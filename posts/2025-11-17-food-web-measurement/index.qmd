---
title: "Food Web Generation and Measurement I"
subtitle: "Links and Detection"
date: 2025-11-17
categories: [community ecology, food webs, hierarchical modeling]
format:
  html:
    toc: true
bibliography: 2025-11-18.bib
---

### Introduction

Food webs are often represented as network graphs, where nodes represent species and links represent observed feeding interactions, or potential feeding interactions inferred from species traits. The [Web of Life](https://www.web-of-life.es/) database houses many such graphs, showcasing their interesting structural patterns and the tremendous efforts ecologists have gone to in order to collect these challenging data.

```{r, echo=F, message=F, warning=F, fig.cap="An example food web originally published by @bascompte2005interaction.", fig.height=4, fig.width=4, dpi=300}
library(tidyverse)
library(tidygraph)
library(ggraph)
library(graphlayouts)
library(igraph)
library(patchwork)

web = read.csv("C:/Users/scaggs.32/OneDrive - The Ohio State University/Professional/sascaggs.github.io/posts/2025-11-17-food-web-measurement/FW_008.csv")

el = web |> 
  rename(sp_row = X) |> 
  gather(key = sp_col, value = link, -sp_row) |> 
  filter(link > 0)

g = graph_from_data_frame(el)

as_tbl_graph(g) |> 
  ggraph(layout = 'focus', focus = 2) + 
  draw_circle(col = '#3300ff', 
              use = 'focus', 
              max.circle = 4) + 
  geom_edge_link0(color = '#00000022') +
  geom_node_point(pch=21, fill='white') + 
  coord_equal() + 
  theme_graph() + 
  theme(plot.margin = unit(c(0,0,0,0),'mm'))
```

```{r, echo=F, message=F, warning=F}
# graphs
graph_theme = theme(
  # panels
  panel.background = element_rect(
    color  = 'black', 
    fill   = '#ffffffff', 
    size   = 1 ), 
  panel.grid = element_blank( ), 
  panel.spacing = unit(15, 'pt'), 
  # axes
  axis.ticks  = element_line(
    color = 'black', 
    size  = 0.5 ), 
  axis.ticks.length = unit(2, 'mm'), 
  axis.text = element_text(color='black'), 
  # strips
  strip.background = element_rect(
    color = '#ffffffff', 
    fill  = '#ffffffff',), 
  strip.text = element_text(
    color  = 'black', 
    vjust  = 1.2,
    hjust  = 0,
    size   = 10, 
    margin = unit( c(4,0,4,0), 'mm') ), 
  # title
  plot.title = element_text(
    size  = 14, 
    hjust = 0, 
    vjust = 3),
  # legend 
  legend.key = element_blank()
)

theme_set(graph_theme)
```

A particular challenge when collecting food web data is knowing whether sampling effort is sufficient. Consider the implications of missing some feeding links. If an interaction involving even a single rare species goes unobserved, it can impact the overall network structure. Since many theories of food webs focus directly on explaining the structure of food webs, measurement models are needed that can help us infer whether the observed structure results from fundamental processes of community assembly or from partial sampling. In this blog post, we explore this measurement problem and develop some preliminary simulations and validations.

To begin, let's lay out a basic model. Let's assume that we want to infer species interactions using a hierarchical model (multiple food webs). This let's us pool information from across multiple webs to understand our ability to detect trophic links.

For each food web $k = 1,...,K$, we observe pairs of species interactions $A^{\text{obs}}_{ij,k} \in \{0,1\}$. A value of $1$ indicates the presence of a link between $i \rightarrow j$. Given this, we model the probability of a link $p_{ij,k}$ as

$$
\begin{equation}
\text{logit}(p_{ij,k}) = \alpha_k + u_{i,k} + v_{j,k}
\end{equation}
$$

where $\alpha_k$ is a baseline interaction rate for web $k$, and $u_{i,k}$ and $v_{j,k}$ are consumer- and resource-specific random effects. We could then say that the "true" set of interactions is Bernoulli distributed

$$
\begin{equation}
A^{\text{obs}}_{ij,k} \sim \text{Bernoulli}(p_{ij,k})
\end{equation}
$$ In other words, there is a probability $p_{ij,k}$ that we observe a link between $ij$ in food web $k$. Such a probability could be directly computed from $A$. But if sampling is biased, then this probability will inherit this bias. For this reason, we also need a detection model.

$$
\begin{equation}
\text{logit}(\pi_k) = \mu_p + \delta_{p,k}
\end{equation}
$$

We assume that each web has its own rate of detection $\pi_k$. The parameter $\mu_k$ is the average log-odds of detection across all food webs, and $\delta_{p,k}$ is a specific deviation of web $k$ from the global average, which we model as non-centered: $\delta_{\pi,k} \sim \mathcal{N}(0,\sigma^2_{\pi})$.

It follows that the probability of observing a link is

$$
\begin{equation}
q_{ij,k} = \pi_k \cdot p_{ij,k}
\end{equation}
$$ We use this to amend our original likelihood to

$$
\begin{equation}
A^{\text{obs}}_{ij,k} \sim \text{Bernoulli}(q_{ij,k})
\end{equation}
$$

In essence, this model assumes that

-   If $A^{\text{true}}_{ij,k} = 0$, then $A^{\text{obs}}_{ij,k} = 0$.
-   If $A^{\text{true}}_{ij,k} = 1$, then we observe $A^{\text{obs}}_{ij,k} = 1$ with probability $\pi_{ij,k}$.

Although false positives are also a possible source of bias, we leave this out of the model, for now.

Finally, we need to declare hierarchical, non-centered priors for all parameters, beginning first with the interaction rates, then the detection probabilities, and finally the random effects. We assume a weakly informative $\mathcal{N}(0,1)$ prior on all latent deviations, and $\text{Exponential}(1)$ for all SD.

### Interaction rates

$$
\begin{align}
\alpha_k &= \mu_{\alpha} + \sigma_{\alpha} \alpha^{(z)}_k \qquad &\text{non-centered} \\
\alpha^{(z)} &\sim \mathcal{N}(0,1) \qquad &\text{latent deviation} \\ 
\mu_{\alpha} &\sim \mathcal{N}(0,1) \qquad &\text{global mean interaction rate} \\ 
\sigma_{\alpha} &\sim \text{Exponential}(1) \qquad &\text{global interaction SD}
\end{align}
$$

### Detection

$$
\begin{align}
\delta_{\pi,k} &= \mu_{\pi} + \sigma_{\pi} \delta^{(z)}_{\pi,k} \qquad &\text{non-centered} \\ 
\delta^{(z)} &\sim \mathcal{N}(0,1) \qquad &\text{latent deviation} \\ 
\mu_{\pi} &\sim \mathcal{N}(0,1) \qquad &\text{global mean detection rate} \\ 
\sigma_{\pi} &\sim \text{Exponential}(1) \qquad &\text{global detection SD}
\end{align}
$$

### Consumer and resource random effects

$$
\begin{align}
u_{i,k} &= \sigma_u u^{(z)}_{i,k}, \qquad &u^{(z)}_{i,k} \sim \mathcal{N}(0,1) \\  
v_{j,k} &= \sigma_v v^{(z)}_{j,k}, \qquad &v^{(z)}_{j,k} \sim \mathcal{N}(0,1) \\ 
\sigma_u &\sim \text{Exponential}(1) \qquad &\sigma_v \sim \text{Exponential}(1)
\end{align}
$$

## Simulation

To test this model, we need to simulate food webs to serve as the "true" sets of interactions, and "observe" our food webs probabilistically from them.

We begin by setting up our parameters.

```{r}
# ---- Setup ----
set.seed(777)
inv_logit = function(x) exp(x) / (1 + exp(x))

# ---- Parameters ----
K = 5   
S = 30
mu_alpha = -2
sd_alpha = 1
sigma_u = 2
sigma_v = 1
mu_pi = -1
sd_pi = 0.5

# generate varying base rates and detection probabilities 
alpha_k = rnorm(K, mu_alpha, sd_alpha); alpha_k
pi_k = inv_logit(rnorm(K, mu_pi, sd_pi)); pi_k
```

We then set up some containers for the simulation results.

```{r}
# ---- Outputs ----
A_true_list = list()
A_obs_list  = list()
```

The basic procedure is the following:

-   Generate $u_{i,k}$ and $v_{j,k}$.
-   Create $S \times S$ matrices for the "true" and "observed" webs.
-   Generate $p_{ij,k}$ based on $\alpha_k$ + random effects.
-   Adjust observed links based on $\pi_k \cdot p_{ij,k}$.

```{r}
for(k in 1:K) {
  
  # Random effects
  u_i = rnorm(S, mean = 0, sd = sigma_u)
  v_j = rnorm(S, mean = 0, sd = sigma_v)
  
  # Matrices 
  A_true = matrix(0, ncol = S, nrow = S)
  A_obs = matrix(0, ncol = S, nrow = S)
  
  # Loop 
  for(i in 1:S) for(j in 1:S) {
    p_ijk = inv_logit(alpha_k[k] + u_i[i] + v_j[j])
    A_true[i,j] = rbinom(1, 1, p_ijk)
    A_obs[i,j] = rbinom(1, 1, pi_k[k] * p_ijk)
    
  }
  A_true_list[[k]] = A_true
  A_obs_list[[k]] = A_obs
}
```

This yields the following summary.

```{r}
dat = data.frame(
  web = 1:K, 
  true_links = sapply(A_true_list, function(A) sum(A)), 
  observed_links = sapply(A_obs_list, function(A) sum(A)), 
  detection = pi_k, 
  alpha = alpha_k
); dat
```

If we examine the degree distributions across all of the webs, we can see how they differ after we adjust for the probability of detection.

```{r, fig.width=6, fig.height=4, message=F, warning=F, fig.cap="The out-degree distribution across all simulated webs."}
deg = function(A) rowSums(A)
data.frame(
  True = unlist(lapply(A_true_list, deg)),
  Observed  = unlist(lapply(A_obs_list,  deg))
) |> 
  gather(key = key, value = degree) |> 
  ggplot(aes(degree)) + 
  geom_histogram(color='white', fill='black') + 
  facet_wrap(~key) + 
  labs(x = 'Outgoing links', y = 'Count')
```

We can see that the degree distribution of the observed links is more zero-inflated and that the tail of the distribution has contracted. Altering the values of $\sigma_u$ and $\sigma_v$ can change these properties to some extent, especially the heaviness of the tail.

```{r, fig.height=4, fig.width=6.5, fig.cap="Examples of true and observed adjacency matrices."}
A_true = A_true_list[[1]]
A_obs = A_obs_list[[1]]

row_sums = rowSums(A_true)

sort_order = order(row_sums)

par(mfrow=c(1,2))
image(A_true[sort_order, sort_order], col = c('black','tomato'), main = 'True', xaxt = 'n', yaxt = 'n')
image(A_obs[sort_order, sort_order],  col = c('black','tomato'), main = 'Observed', xaxt = 'n', yaxt = 'n')
```

## Non-Random Links

Now that we have a basic model and simulation skeleton, we should consider which traits might lead to a species having more or less feeding links. For example, we could expect that larger species may have broader diets. This idea has be presented many times, with notable examples coming from @petchey2008size and @allesina2011predicting. Here we can make a simplistic assumption that the mass of a consumer $M_i$ can have a linear, additive effect on the probability of a link.[^1]

[^1]: @allesina2011predicting explore additional possibilities, such as the *Ratio* model, that we discuss in more detail in the next post.

$$
\begin{equation}
\text{logit}(p_{ij,k}) = \alpha_k + (u_{i,k} + \beta_M M_i) + v_{j,k}
\end{equation}
$$

We can now compare our original simulations to a second pair where we adjust the probability based on body size. 

```{r}
# ---- Setup ----
set.seed(777)
inv_logit = function(x) exp(x) / (1 + exp(x))

# ---- Parameters ----
K = 5   
S = 30
mu_alpha = -2
sd_alpha = 1
sigma_u = 2
bM = 1.2
mass = exp(rnorm(S,0,3))
M = log(mass)
sigma_v = 1
mu_pi = -1
sd_pi = 0.5

# generate varying base rates and detection probabilities 
alpha_k = rnorm(K, mu_alpha, sd_alpha)
pi_k = inv_logit(rnorm(K, mu_pi, sd_pi))

# ---- Outputs ----
A_true_list = list()
A_obs_list  = list()
A_Mtrue_list = list()
A_Mobs_list = list()


for(k in 1:K) {
  
  # Random effects
  u_i = rnorm(S, mean = 0, sd = sigma_u)
  v_j = rnorm(S, mean = 0, sd = sigma_v)
  
  # Matrices 
  A_true = matrix(0, ncol = S, nrow = S)
  A_obs = matrix(0, ncol = S, nrow = S)
  A_Mtrue = matrix(0, ncol = S, nrow = S)
  A_Mobs = matrix(0, ncol = S, nrow = S)
  
  # Loop 
  for(i in 1:S) for(j in 1:S) {
    p_ijk = inv_logit(alpha_k[k] + u_i[i] + v_j[j])
    A_true[i,j] = rbinom(1, 1, p_ijk)
    A_obs[i,j] = rbinom(1, 1, pi_k[k] * p_ijk)
    
    # body mass model 
    .p_ijk = inv_logit(alpha_k[k] + (u_i[i] + bM * M[i]) + v_j[j])
    A_Mtrue[i,j] = rbinom(1, 1, .p_ijk)
    A_Mobs[i,j] = rbinom(1, 1, pi_k[k] * .p_ijk)
    
  }
  A_true_list[[k]] = A_true
  A_obs_list[[k]] = A_obs
  A_Mtrue_list[[k]] = A_Mtrue
  A_Mobs_list[[k]] = A_Mobs
}
```

Clearly the choice of the distribution of body masses will have an influence over this simulation. Here I assume that body masses are heavy-tailed and that we model them on a log-scale. For example, the code above gives the following: 

```{r, fig.height=3.75, fig.width=6.5, dpi-300}
library(latex2exp)

data.frame(mass) |>  
  ggplot(aes(x=mass)) +  
  stat_density(adjust = 0.5) + 
  labs(x=expression(M[i])) + 
  ggtitle('Raw mass') + 
  ylim(c(0,0.16)) + 
  
data.frame(M) |>  
  ggplot(aes(x=M)) +  
  stat_density(adjust = 0.5) + 
  labs(x=expression(log(M[i]))) + 
  ggtitle('Log transformed') + 
  ylim(c(0,0.16))

```


```{r, fig.width=4, fig.height=5}
A_true = A_true_list[[1]]
A_obs = A_obs_list[[1]]
A_Mtrue = A_Mtrue_list[[1]]
A_Mobs = A_Mobs_list[[1]]

row_sums = rowSums(A_true)
sort_order = order(row_sums)

par(mfrow=c(2,2), mar=c(3,1,3,1))
image(A_true[sort_order, sort_order], col = c('black','tomato'), main = 'True', xaxt = 'n', yaxt = 'n')
image(A_obs[sort_order, sort_order],  col = c('black','tomato'), main = 'Observed', xaxt = 'n', yaxt = 'n')

row_sums = rowSums(A_Mtrue)
sort_order = order(row_sums)

image(A_Mtrue[sort_order, sort_order], col = c('black','tomato'), main = 'True (Allometric)', xaxt = 'n', yaxt = 'n')
image(A_Mobs[sort_order, sort_order],  col = c('black','tomato'), main = 'Observed (Allometric)', xaxt = 'n', yaxt = 'n')
```

The effect of body mass concentrates the true links toward the largest species. It is also gravitational: energy, in the form of biomass, flows toward these largest species. The effect of this on the observed links is very subtle but not entirely absent. 

```{r, fig.width=6.5, fig.height=6, dpi=300}
lo = layout_as_tree(graph_from_adjacency_matrix(A_Mobs_list[[1]]))
par(mfrow=c(2,2), mar=c(3,1,3,1))
plot(graph_from_adjacency_matrix(A_true_list[[1]]),  
     edge.arrow.size=0.2, 
     vertex.color = 'white', 
     vertex.label = NA, 
     layout = lo, 
     main = 'True')
plot(graph_from_adjacency_matrix(A_obs_list[[1]]),   
     edge.arrow.size=0.2, 
     vertex.color = 'white', 
     vertex.label = NA, 
     layout = lo, 
     main = 'Observed')
plot(graph_from_adjacency_matrix(A_Mtrue_list[[1]]), 
     edge.arrow.size=0.2, 
     vertex.color = 'white', 
     vertex.label = NA, 
     layout = lo, 
     main = 'True (Allometric)')
plot(graph_from_adjacency_matrix(A_Mobs_list[[1]]),  
     edge.arrow.size=0.2, 
     vertex.color = 'white', 
     vertex.label = NA, 
     layout = lo, 
     main = 'Observed (Allometric)')
```

