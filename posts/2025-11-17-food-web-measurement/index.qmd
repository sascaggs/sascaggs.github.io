---
title: "Food Web Generation and Measurement I"
subtitle: "Links and Detection"
date: 2025-11-17
categories: [community ecology, food webs, hierarchical modeling]
format:
  html:
    toc: true
bibliography: 2025-11-18.bib
---

### Introduction

Food webs are often represented as network graphs, where nodes represent species and links represent observed feeding interactions, or potential feeding interactions inferred from species traits. The [Web of Life](https://www.web-of-life.es/) database houses many such graphs, showcasing their interesting structural patterns and the tremendous efforts ecologists have gone to in order to collect these challenging data.

```{r, echo=F, message=F, warning=F, fig.cap="An example food web originally published by @bascompte2005interaction.", fig.height=2, fig.width=2, dpi=300}
library(tidyverse)
library(tidygraph)
library(ggraph)
library(graphlayouts)
library(igraph)
library(patchwork)
library(latex2exp)

web = read.csv("C:/Users/scaggs.32/OneDrive - The Ohio State University/Professional/sascaggs.github.io/posts/2025-11-17-food-web-measurement/FW_008.csv")

el = web |> 
  rename(sp_row = X) |> 
  gather(key = sp_col, value = link, -sp_row) |> 
  filter(link > 0)

g = graph_from_data_frame(el)
```

```{r, echo=F, message=F, warning=F}
# graphs
graph_theme = theme(
  # panels
  panel.background = element_rect(
    color  = 'black', 
    fill   = '#ffffffff', 
    size   = 1 ), 
  panel.grid = element_blank( ), 
  panel.spacing = unit(15, 'pt'), 
  # axes
  axis.ticks  = element_line(
    color = 'black', 
    size  = 0.5 ), 
  axis.ticks.length = unit(2, 'mm'), 
  axis.text = element_text(color='black'), 
  # strips
  strip.background = element_rect(
    color = '#ffffffff', 
    fill  = '#ffffffff',), 
  strip.text = element_text(
    color  = 'black', 
    vjust  = 1.2,
    hjust  = 0,
    size   = 10, 
    margin = unit( c(4,0,4,0), 'mm') ), 
  # title
  plot.title = element_text(
    size  = 14, 
    hjust = 0, 
    vjust = 3),
  # legend 
  legend.key = element_blank()
)

theme_set(graph_theme)
```

```{r, include=F}
as_tbl_graph(g) |> 
  ggraph(layout = 'focus', focus = 2) + 
  draw_circle(col = '#3300ff', 
              use = 'focus', 
              max.circle = 4) + 
  geom_edge_link0(color = '#00000022') +
  geom_node_point(pch=21, fill='white') + 
  coord_equal() + 
  theme_graph() + 
  theme(plot.margin = unit(c(0,0,0,0),'mm'))
```

A particular challenge when collecting food web data is knowing whether sampling effort is sufficient. Consider the implications of missing some feeding links. If an interaction involving even a single rare species goes unobserved, it can impact the overall network structure. Since many theories of food webs focus directly on explaining the structure of food webs, measurement models are needed that can help us infer whether the observed structure results from fundamental processes of community assembly or from partial sampling. In this blog post, we explore this measurement problem and develop some preliminary simulations and validations.

To begin, let's lay out a basic model. Let's assume that we want to infer species interactions using a hierarchical model (multiple food webs). This let's us pool information from across multiple webs to understand our ability to detect trophic links.

For each food web $k = 1,...,K$, we observe pairs of species interactions $A^{\text{obs}}_{ij,k} \in \{0,1\}$. A value of $1$ indicates the presence of a link between $i \rightarrow j$. Given this, we model the probability of a link $\pi_{ij,k}$ as

$$
\begin{equation}
\text{logit}(\pi_{ij,k}) = \alpha_k + u_{i,k} + v_{j,k}
\end{equation}
$$

where $\alpha_k$ is a baseline interaction rate for web $k$, and $u_{i,k}$ and $v_{j,k}$ are consumer- and resource-specific random effects. We could then say that the "true" set of interactions is Bernoulli distributed

$$
\begin{equation}
A^{\text{obs}}_{ij,k} \sim \text{Bernoulli}(\pi_{ij,k})
\end{equation}
$$ In other words, there is a probability $\pi_{ij,k}$ that we observe a link between $ij$ in food web $k$. Such a probability could be directly computed from $A$. But if sampling is biased, then this probability will inherit this bias. For this reason, we also need a detection model.

$$
\begin{equation}
\text{logit}(p_k) = \mu_p + \sigma_p p^{(z)}_k
\end{equation}
$$

We assume that each web has its own rate of detection $p_k$. The parameter $\mu_p$ is the average log-odds of detection across all food webs with a specific deviation of web $k$ from the global average, which we model as non-centered: $\sigma_p p^{(z)}_k$.

It follows that the probability of observing a link is

$$
\begin{equation}
q_{ij,k} = p_k \cdot \pi_{ij,k}
\end{equation}
$$ We use this to amend our original likelihood to

$$
\begin{equation}
A^{\text{obs}}_{ij,k} \sim \text{Bernoulli}(q_{ij,k})
\end{equation}
$$

In essence, this model assumes that

-   If $A^{\text{true}}_{ij,k} = 0$, then $A^{\text{obs}}_{ij,k} = 0$.
-   If $A^{\text{true}}_{ij,k} = 1$, then we observe $A^{\text{obs}}_{ij,k} = 1$ with probability $\pi_{ij,k}$.

Although false positives are also a possible source of bias, we leave this out of the model, for now.

Finally, we need to declare hierarchical, non-centered priors for all parameters, beginning first with the interaction rates, then the detection probabilities, and finally the random effects. We assume a weakly informative $\mathcal{N}(0,1)$ prior on all latent deviations, and $\text{Exponential}(1)$ for all SD.

### Interaction rates

$$
\begin{align}
\pi_{ij,k} &= \text{logit}^{-1}(\alpha_k + u_{i,k} + v_{j,k}) \qquad &\text{true link probability} \\
\alpha_k &= \mu_{\alpha} + \sigma_{\alpha} \alpha^{(z)}_k \qquad &\text{non-centered} \\
\alpha^{(z)} &\sim \mathcal{N}(0,1) \qquad &\text{latent deviation} \\ 
\mu_{\alpha} &\sim \mathcal{N}(0,1) \qquad &\text{global mean interaction rate} \\ 
\sigma_{\alpha} &\sim \text{Exponential}(1) \qquad &\text{global interaction SD}
\end{align}
$$

### Detection

$$
\begin{align}
p_k &= \text{logit}^{-1}(\mu_{p} + \sigma_{p} p^{(z)}_{k}) \qquad &\text{non-centered} \\ 
p^{(z)}_k &\sim \mathcal{N}(0,1) \qquad &\text{latent deviation} \\ 
\mu_{p} &\sim \mathcal{N}(0,1) \qquad &\text{global mean detection rate} \\ 
\sigma_{p} &\sim \text{Exponential}(1) \qquad &\text{global detection SD}
\end{align}
$$

### Consumer and resource random effects

$$
\begin{align}
u_{i,k} &= \sigma_u u^{(z)}_{i,k}, \qquad &u^{(z)}_{i,k} \sim \mathcal{N}(0,1) \\  
v_{j,k} &= \sigma_v v^{(z)}_{j,k}, \qquad &v^{(z)}_{j,k} \sim \mathcal{N}(0,1) \\ 
\sigma_u &\sim \text{Exponential}(1) \qquad &\sigma_v \sim \text{Exponential}(1)
\end{align}
$$

## Simulation

To test this model, we need to simulate food webs to serve as the "true" sets of interactions, and "observe" our food webs probabilistically from them. Here are the packages used in this workflow.

```{r, eval=F}
library(tidyverse)
library(tidygraph)
library(ggraph)
library(graphlayouts)
library(igraph)
library(patchwork)
library(latex2exp)
```

We begin by setting up our parameters.

```{r}
# ---- Setup ----
set.seed(777)
inv_logit = function(x) exp(x) / (1 + exp(x))

# ---- Parameters ----
K = 3   
S = 30
mu_alpha = -2
sigma_alpha = 1
sigma_u = 2
sigma_v = 1
mu_p = -1
sigma_p = 0.5

# generate varying base rates and detection probabilities 
alpha_k = rnorm(K, mu_alpha, sigma_alpha); alpha_k
p_k = inv_logit(rnorm(K, mu_p, sigma_p)); p_k
```

We then set up some containers for the simulation results.

```{r}
# ---- Outputs ----
A_true = array(0, dim = c(S,S,K))
A_obs  = array(0, dim = c(S,S,K))
```

The basic procedure is the following:

-   Generate $u_{i,k}$ and $v_{j,k}$.
-   Create $S \times S$ matrices for the "true" and "observed" webs.
-   Generate $\pi_{ij,k}$ based on $\alpha_k$ + random effects.
-   Adjust observed links based on $p_k \cdot \pi_{ij,k}$.

```{r}
for(k in 1:K) {
  
  # Random effects
  u_i = rnorm(S, mean = 0, sd = sigma_u)
  v_j = rnorm(S, mean = 0, sd = sigma_v)
  
  # Loop 
  for(i in 1:S) for(j in 1:S) {
    
    pi_ijk = inv_logit(alpha_k[k] + u_i[i] + v_j[j])
    
    A_true[i,j,k] = rbinom(1, 1, pi_ijk)
    A_obs[i,j,k] = rbinom(1, 1, p_k[k] * pi_ijk)
  }
}
```

This yields the following summary.

```{r}
dat = data.frame(
  web            = 1:K, 
  true_links     = apply(A_true, 3, sum), 
  observed_links = apply(A_obs,  3, sum), 
  detection      = p_k, 
  alpha          = alpha_k
); dat
```

If we examine the degree distributions across all of the webs, we can see how they differ after we adjust for the probability of detection.

```{r, fig.width=6, fig.height=4, message=F, warning=F, fig.cap="The out-degree distribution across all simulated webs."}
deg = function(A) apply(A, 3, function(mat) rowSums(mat)) |>  as.vector()

data.frame(
  True      = deg(A_true),
  Observed  = deg(A_obs)
) |> 
  gather(key = key, value = degree) |> 
  ggplot(aes(degree)) + 
  geom_histogram(color='white', fill='black') + 
  facet_wrap(~key) + 
  labs(x = 'Outgoing links', y = 'Count')
```

We can see that the degree distribution of the observed links is more zero-inflated and that the tail of the distribution has contracted. Altering the values of $\sigma_u$ and $\sigma_v$ can change these properties to some extent, especially the heaviness of the tail.

```{r, fig.height=4, fig.width=6.5, fig.cap="Examples of true and observed adjacency matrices."}
A_true1 = A_true[,,1]
A_obs1  = A_obs[,,1]

row_sums = rowSums(A_true1)
sort_order = order(row_sums)

par(mfrow=c(1,2))
image(A_true1[sort_order, sort_order], 
      col = c('black','tomato'), 
      main = 'True', 
      xaxt = 'n', yaxt = 'n')
image(A_obs1[sort_order, sort_order],  
      col = c('black','tomato'), 
      main = 'Observed', 
      xaxt = 'n', yaxt = 'n')
```

### Model Validation

The next step is to develop the hierarchical model in Stan and validate it using the true and observed interaction matrices. First, we define the model block to mirror our simulation and the model notation. This block include the priors for all parameters and hyperparameters.

``` stan
model {
  // priors for latent z-parameters 
  p_z ~ normal(0,1); 
  alpha_z ~ normal(0,1);
  to_vector(u_z) ~ normal(0,1); 
  to_vector(v_z) ~ normal(0,1); 
  
  //priors for hyperparameters 
  mu_p ~ normal(0,1);
  sigma_p ~ exponential(1);
  mu_alpha ~ normal(0,1);
  sigma_alpha ~ exponential(1); 
  sigma_u ~ exponential(1);
  sigma_v ~ exponential(1);
  
  // likelihood 
  for(k in 1:K) {
    // detection random effect
    real p_k = inv_logit(mu_p + sigma_p * p_z[k]);
    
    // link random effect
    real alpha_k = mu_alpha + sigma_alpha * alpha_z[k]; 
    
    for(i in 1:N_cons) {
      // consumer random effects 
      real u_ik = sigma_u * u_z[i,k];
      
      for(j in 1:N_res) {
        // resource random effect 
        real v_jk = sigma_v * v_z[j,k];
        
        // link probability 
        real pi_ijk = inv_logit(alpha_k + u_ik + v_jk);
        
        // detection adjusted probability 
        real q_ijk = p_k * pi_ijk;
        
        // likelihood
        A[i,j,k] ~ bernoulli(q_ijk);
      }
    }
  }
}
```

For all parameters that have priors, we declare them in the parameters block.

``` stan
parameters {
  vector[K] p_z;
  vector[K] alpha_z;
  matrix[N_cons, K] u_z;
  matrix[N_res, K] v_z;
  
  real mu_p; 
  real<lower=0> sigma_p; 
  real mu_alpha; 
  real<lower=0> sigma_alpha; 
  real<lower=0> sigma_u; 
  real<lower=0> sigma_v;
}
```

Finally, we set up the data needed for the model.

```{r}
data_list = list(
  N_cons = S, 
  N_res  = S, 
  K      = K, 
  A      = A_obs
)
```

This corresponds to

``` stan
data {
  int<lower=1> N_cons;
  int<lower=1> N_res;
  int<lower=1> K;
  // observed adjacency
  array[N_cons, N_res, K] int<lower=0, upper=1> A; 
}
```

View the full stan model.

``` stan

data {
  int<lower=1> N_cons;
  int<lower=1> N_res;
  int<lower=1> K;
  // observed adjacency
  array[N_cons, N_res, K] int<lower=0, upper=1> A; 
}


parameters {
  // non-centered latent effects
  vector[K] p_z;
  vector[K] alpha_z;
  matrix[N_cons, K] u_z;
  matrix[N_res, K] v_z;
  
  // hyperparameters
  real mu_p; 
  real<lower=0> sigma_p; 
  real mu_alpha; 
  real<lower=0> sigma_alpha; 
  real<lower=0> sigma_u; 
  real<lower=0> sigma_v;
}

model {
  // priors for latent z-parameters 
  p_z ~ normal(0,1); 
  alpha_z ~ normal(0,1);
  to_vector(u_z) ~ normal(0,1); 
  to_vector(v_z) ~ normal(0,1); 
  
  //priors for hyperparameters 
  mu_p ~ normal(0,1);
  sigma_p ~ exponential(1);
  mu_alpha ~ normal(0,1);
  sigma_alpha ~ exponential(1); 
  sigma_u ~ exponential(1);
  sigma_v ~ exponential(1);
  
  // likelihood 
  for(k in 1:K) {
    // detection random effect
    real p_k = inv_logit(mu_p + sigma_p * p_z[k]);
    
    // link random effect
    real alpha_k = mu_alpha + sigma_alpha * alpha_z[k]; 
    
    for(i in 1:N_cons) {
      // consumer random effects 
      real u_ik = sigma_u * u_z[i,k];
      
      for(j in 1:N_res) {
        // resource random effect 
        real v_jk = sigma_v * v_z[j,k];
        
        // link probability 
        real pi_ijk = inv_logit(alpha_k + u_ik + v_jk);
        
        // detection adjusted probability 
        real q_ijk = p_k * pi_ijk;
        
        // likelihood
        A[i,j,k] ~ bernoulli(q_ijk);
      }
    }
  }
}
```

After saving this model, we fit it using `cmdstanr` like so. This model samples in under a minute with 4 parallel chains when `S = 30` and `K = 3`.

```{r, eval=F}
library(cmdstanr)
mod <- cmdstan_model("detection_model.stan")

fit1 <- mod$sample(
  data = data_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  adapt_delta = 0.95
)

fit$save_output_files()
```

```{r, echo=F, eval=T, message=F, warning=F}
library(cmdstanr)
#mod <- cmdstan_model("C:/Users/scaggs.32/OneDrive - The Ohio State University/Professional/sascaggs.github.io/posts/2025-11-17-food-web-measurement/detection_model.stan")

fit1 = readRDS('fits/detection_model1.RDS')

```

Our primary task is to determine whether the model can recover the parameters that we used within the simulation. Two focal parameters are the baseline interaction probability $\alpha_k$ and the detection probability $p_k$. As a reminder, we generated these values by setting a $\mu*$ and $\sigma*$ for each and then drawing them `K` times.

```{r}
focal_pars = data.frame(k = factor(1:k), alpha_k, p_k)
focal_pars
```

Since we modeled these as non-centered parameters, we need to reconstruct them from the draws. We will do this using the `tidybayes` package for handling Bayesian models in R.[^1] Let's start with $\alpha_k$. We will generate a predicted $\hat{\alpha}_k$ by pulling out `alpha_z[k]`, `mu_alpha`, and `sigma_alpha` and the computing `alpha_k_hat`.

[^1]: Details of how to use `tidybayes` can be found [here](https://mjskay.github.io/tidybayes/articles/tidybayes.html).

```{r, warning=F, message=F}
library(tidybayes)

alpha_k_draws = fit1 |> 
  spread_draws(mu_alpha, sigma_alpha, alpha_z[k]) |>
  group_by(k) |> 
  mutate(alpha_k_hat = mu_alpha + sigma_alpha * alpha_z) 
```

We can summarize the posterior distribution in many ways. Here I will plot this distribution and overlay the original values.

```{r, fig.width=4.25, fig.height=4, echo=F}
#| fig.cap: "Comparison of predicted $\\hat{\\alpha}_k$ to the input values of $\\alpha_k$ (magenta $*$). Points are posterior means, and bars are 67\\% and 90\\% highest density posterior intervals."

alpha_k_draws |> 
  ggplot(aes(x=alpha_k_hat, y=factor(k))) + 
  stat_pointinterval(color='black', .width=c(0.67,0.9)) + 
  labs(x = TeX("$\\hat{\\alpha}_k$"), y = 'k') + 
  geom_point(data=focal_pars, 
             aes(x = alpha_k, y=k), 
             color = '#ffffff', pch=8, size=3, stroke=3) +
  geom_point(data=focal_pars, 
             aes(x = alpha_k, y=k), 
             color = 'magenta', pch=8, size=2, stroke=2)
```

$\hat{\alpha}_k$ falls within the density intervals in all cases, and comes extremely close to the mean for webs 1 and 3. We can now do this same to compute $\hat{p}_k$, though in this case we need to transform the model parameters from the log-odds scale to a probability scale.

```{r, fig.width=4.25, fig.height=4}
#| fig.cap: "Comparison of predicted $\\hat{p}_k$ to the input values of $p_k$ (cyan $*$). Points are posterior means, and bars are 67\\% and 90\\% highest density posterior intervals."

p_k_draws = fit1 |> 
  spread_draws(p_z[k], sigma_p, mu_p) |> 
  group_by(k) |> 
  mutate(p_k_hat = inv_logit(mu_p + sigma_p*p_z))

p_k_draws |> 
  ggplot(aes(x=p_k_hat, y=factor(k))) + 
  stat_pointinterval(color='black', .width=c(0.67,0.9)) + 
  labs(x = TeX("$\\hat{p}_k$"), y = 'k') + 
  geom_point(data=focal_pars, 
             aes(x = p_k, y=k), 
             color = 'white', pch=8, size=4, stroke=2) +
  geom_point(data=focal_pars, 
             aes(x = p_k, y=k), 
             color = 'cyan3', pch=8, size=2, stroke=2)
```

Again we come very close to recovering the true parameters, with the biggest difference being in web 2. Although we can certainly be more precise, this is still a good sign that the model could, in principle, estimate the latent detection probability $p_k$.

## Non-Random Links

Now that we have a basic model and simulation skeleton, we should consider which traits might lead to a species having more or less feeding links. For example, we could expect that larger species may have broader diets. This idea has be presented many times, with notable examples coming from @petchey2008size and @allesina2011predicting. Here we can make a simplistic assumption that the mass of a consumer $M_i$ can have a linear, additive effect on the probability of a link.[^2]

[^2]: @allesina2011predicting explore additional possibilities, such as the *Ratio* model, that we discuss in more detail in the next post.

$$
\begin{equation}
\text{logit}(\pi_{ij,k}) = \alpha_k + (u_{i,k} + b M_i) + v_{j,k}
\end{equation}
$$

Rather than having a single parameter $b$ for all webs, it is more likely that we have distinct allometric relationships per web, because each simulated web could represent a distinct ecological community. To do this, we fit a separate fixed slope for each web $k$

$$
\begin{align}
\text{logit}(\pi_{ij,k}) &= \alpha_k + (u_{i,k} + b_k M_{i,k}) + v_{j,k}, \qquad b_k \sim \mathcal{N}(1,1) 
\end{align}
$$

```{r}
# ---- Setup ----
set.seed(777)
inv_logit = function(x) exp(x) / (1 + exp(x))

# ---- Parameters ----
K = 3   
S = 30
mu_alpha = -2
sigma_alpha = 1
sigma_u = 2
sigma_v = 1
mu_p = -1
sigma_p = 0.5
mu_b = 1
sigma_b = 0.3

# consumer mass 
M = matrix(NA_real_, nrow = S, ncol = K)

# generate varying base rates and detection probabilities, and body mass effects 
alpha_k = rnorm(K, mu_alpha, sigma_alpha)
p_k = inv_logit(rnorm(K, mu_p, sigma_p))
b_k = rnorm(K, mu_b, sigma_b)

# ---- Outputs ----
A_true = array(0, dim = c(S,S,K))
A_obs  = array(0, dim = c(S,S,K))
A_Mtrue = array(0, dim = c(S,S,K))
A_Mobs  = array(0, dim = c(S,S,K))


for(k in 1:K) {
  # Random effects
  u_i = rnorm(S, mean = 0, sd = sigma_u)
  v_j = rnorm(S, mean = 0, sd = sigma_v)
  
  
    # Mass 
    mass_k = exp(rnorm(S, mu_b, sigma_b))
    M[,k] = log(mass_k)
  
  # Loop 
  for(i in 1:S) for(j in 1:S) {
    
    pi_ijk = inv_logit(alpha_k[k] + u_i[i] + v_j[j])
    A_true[i,j,k] = rbinom(1, 1, pi_ijk)
    A_obs[i,j,k] = rbinom(1, 1, p_k[k] * pi_ijk)
    
    # body mass model 
    .pi_ijk = inv_logit(alpha_k[k] + (u_i[i] + b_k[k] * M[i,k]) + v_j[j])
    A_Mtrue[i,j,k] = rbinom(1, 1, .pi_ijk)
    A_Mobs[i,j,k] = rbinom(1, 1, p_k[k] * .pi_ijk)
    
  }
}
```

Clearly the choice of the distribution of body masses will have an influence over this simulation. Here I assume that body masses are heavy-tailed and that we model them on a log-scale. For example, the code above gives the following:

```{r, fig.height=3.75, fig.width=6.5, dpi-300}
data.frame(mass_k) |>  
  ggplot(aes(x=mass_k)) +  
  stat_density(adjust = 0.5) + 
  labs(x=expression(M[i])) + 
  ggtitle('Raw mass') + 

data.frame(M=M[,1]) |>  
  ggplot(aes(x=M)) +  
  stat_density(adjust = 0.5) + 
  labs(x=expression(log(M[i]))) + 
  ggtitle('Log transformed') 
```

```{r, fig.width=4, fig.height=5}
A_true1  = A_true[,,1]
A_obs1   = A_obs[,,1]
A_Mtrue1 = A_Mtrue[,,1]
A_Mobs1  = A_Mobs[,,1]

row_sums = rowSums(A_true1)
sort_order = order(row_sums)

par(mfrow=c(2,2), mar=c(3,1,3,1))
image(A_true1[sort_order, sort_order], col = c('black','tomato'), main = 'True', xaxt = 'n', yaxt = 'n')
image(A_obs1[sort_order, sort_order],  col = c('black','tomato'), main = 'Observed', xaxt = 'n', yaxt = 'n')

row_sums = rowSums(A_Mtrue1)
sort_order = order(row_sums)

image(A_Mtrue1[sort_order, sort_order], col = c('black','tomato'), main = 'True (Allometric)', xaxt = 'n', yaxt = 'n')
image(A_Mobs1[sort_order, sort_order],  col = c('black','tomato'), main = 'Observed (Allometric)', xaxt = 'n', yaxt = 'n')
```

The effect of body mass concentrates the true links toward the largest species. It is also gravitational: energy, in the form of biomass, flows toward these largest species. The effect of this on the observed links is very subtle but not entirely absent.

```{r, fig.width=6.5, fig.height=6, dpi=300, warning=F}
lo = layout_as_tree(graph_from_adjacency_matrix(A_Mobs1))
par(mfrow=c(2,2), mar=c(3,1,3,1))
plot(graph_from_adjacency_matrix(A_true1),  
     edge.arrow.size=0.2, 
     vertex.color = 'white', 
     vertex.label = NA, 
     layout = lo, 
     main = 'True')
plot(graph_from_adjacency_matrix(A_obs1),   
     edge.arrow.size=0.2, 
     vertex.color = 'white', 
     vertex.label = NA, 
     layout = lo, 
     main = 'Observed')
plot(graph_from_adjacency_matrix(A_Mtrue1), 
     edge.arrow.size=0.2, 
     vertex.color = 'white', 
     vertex.label = NA, 
     layout = lo, 
     main = 'True (Allometric)')
plot(graph_from_adjacency_matrix(A_Mobs1),  
     edge.arrow.size=0.2, 
     vertex.color = 'white', 
     vertex.label = NA, 
     layout = lo, 
     main = 'Observed (Allometric)')
```

### Stan model

Now we adjust the original Stan model to include these biomass effects. The main changes, besides adding the non-centered effects of $b_k$, are to include the body mass matrix `M`.

``` stan


data {
  int<lower=1> N_cons;
  int<lower=1> N_res;
  int<lower=1> K;
  matrix[N_cons,K] M;
  // observed adjacency
  array[N_cons, N_res, K] int<lower=0, upper=1> A; 
}


parameters {
  // body mass slopes 
  vector[K] b_k;
  //real mu_b;
  //real<lower=0> sigma_b;
  
  // non-centered latent effects
  vector[K] p_z;
  vector[K] alpha_z;
  matrix[N_cons, K] u_z;
  matrix[N_res, K] v_z;
  
  // hyperparameters
  real mu_p; 
  real<lower=0> sigma_p; 
  real mu_alpha; 
  real<lower=0> sigma_alpha; 
  real<lower=0> sigma_u; 
  real<lower=0> sigma_v;
}

model {
  // b priors
  //b_z ~ normal(0,1); // if random slopes, use these
  //mu_b ~ normal(0,1); 
  //sigma_b ~ exponential(1); // else 
  b_k ~ normal(1,1);
  
  // priors for latent z-parameters 
  p_z ~ normal(0,1); 
  alpha_z ~ normal(0,1);
  to_vector(u_z) ~ normal(0,1); 
  to_vector(v_z) ~ normal(0,1); 
  
  //priors for hyperparameters 
  mu_p ~ normal(0,1);
  sigma_p ~ exponential(1);
  mu_alpha ~ normal(0,1);
  sigma_alpha ~ exponential(1); 
  sigma_u ~ exponential(1);
  sigma_v ~ exponential(1);
  
  // likelihood 
  for(k in 1:K) {
    // detection random effect
    real p_k = inv_logit(mu_p + sigma_p * p_z[k]);
    
    // link random effect
    real alpha_k = mu_alpha + sigma_alpha * alpha_z[k];
    
    // body mass slopes 
    //real b_k = mu_b + sigma_b * b_z[k];
    
    for(i in 1:N_cons) {
      // consumer random effects 
      real u_ik = sigma_u * u_z[i,k];
      
      for(j in 1:N_res) {
        // resource random effect 
        real v_jk = sigma_v * v_z[j,k];
        
        // link probability 
        real pi_ijk = inv_logit(alpha_k + (u_ik + b_k[k]*M[i,k]) + v_jk);
        
        // detection adjusted probability 
        real q_ijk = p_k * pi_ijk;
        
        // likelihood
        A[i,j,k] ~ bernoulli(q_ijk);
      }
    }
  }
}

```

As before, we fit the model but with slightly increased `adapt_delta` to accommodate.

```{r, eval=F}
Bmod <- cmdstan_model("detection_model_bodymass.stan")

fit2 <- modB$sample(
  data = data_list,
  seed = 123,
  chains = 4,
  parallel_chains = 4,
  adapt_delta = 0.95
)

fit2$save_output_files()
```

```{r, echo=F, message=F, warning=F}
fit2 = readRDS('fits/detection_model_bodymass.RDS')
```

As before, we want to compare the predicted values of $\hat{\alpha}_k$, $\hat{p}_k$, and $\hat{b}_k$ to their input values.

```{r}
focal_pars2 = data.frame(k = factor(1:k), 
                        alpha_k, p_k, b_k)
focal_pars2
```

```{r, echo=F, fig.width=6.5, fig.height=3.5}

#| fig.cap: "A. Comparison of predicted $\\hat{\\alpha}_k$ to the input values $\\alpha_k$. B. Comparison of predicted $\\hat{p}_k$ to the input values $p_k$."

alpha_k_draws2 = fit2 |> 
  spread_draws(mu_alpha, sigma_alpha, alpha_z[k]) |>
  group_by(k) |> 
  mutate(alpha_k_hat = mu_alpha + sigma_alpha * alpha_z) 

p1 = alpha_k_draws2 |> 
  ggplot(aes(x=alpha_k_hat, y=factor(k))) + 
  stat_pointinterval(color='black', .width=c(0.67,0.9)) + 
  labs(x = TeX("$\\hat{\\alpha}_k$"), y = 'k') + 
  geom_point(data=focal_pars2, 
             aes(x = alpha_k, y=k), 
             color = 'white', pch=8, size=4, stroke=2) +
  geom_point(data=focal_pars2, 
             aes(x = alpha_k, y=k), 
             color = 'magenta', pch=8, size=2, stroke=2)


p_k_draws2 = fit2 |> 
  spread_draws(p_z[k], sigma_p, mu_p) |> 
  group_by(k) |> 
  mutate(p_k_hat = inv_logit(mu_p + sigma_p*p_z))

p2 = p_k_draws2 |> 
  ggplot(aes(x=p_k_hat, y=factor(k))) + 
  stat_pointinterval(color='black', .width=c(0.67,0.9)) + 
  labs(x = TeX("$\\hat{p}_k$"), y = 'k') + 
  geom_point(data=focal_pars2, 
             aes(x = p_k, y=k), 
             color = 'white', pch=8, size=4, stroke=2) +
  geom_point(data=focal_pars, 
             aes(x = p_k, y=k), 
             color = 'cyan3', pch=8, size=2, stroke=2)

p1 + p2 + plot_annotation(tag_levels = 'A')
```

Again we see a decent concordance between the predicted and input values, although in some cases we are near the edge of the highest density posterior interval.

Next, we examine the predicted $\hat{b}_k$ from the posterior against the input values.

```{r, echo=F}
#| fig.cap: "Comparison of the predicted $\\hat{b}_k$ values against the input values $b_k$."

b_k_draws2 = fit2 |> 
  spread_draws(b_k[k]) 

b_k_draws2 |> 
  ggplot(aes(x=b_k, y=factor(k))) + 
  stat_pointinterval(color='black', .width=c(0.67,0.9)) + 
  labs(x = TeX("$\\hat{b}_k$"), y = 'k') + 
  geom_point(data=focal_pars2, 
             aes(x = b_k, y=k), 
             color = 'white', pch=8, size=4, stroke=2) +
  geom_point(data=focal_pars, 
             aes(x = b_k, y=k), 
             color = '#3300ff', pch=8, size=2, stroke=2)

```

Treat each slope $b_k$ as a fixed effect lets us recover the input parameters well. Alternatively, if one uses a random slope where $b_k = \mu_b + \sigma_b b^{(z)}_k$, the partial pooling in this model can drag the slope parameter toward 0. This is because $\mu_b$ is essentially competing with other random effects for the same variation. It is also sensible to assume that there would not be any partially pooling across $b_k$.
