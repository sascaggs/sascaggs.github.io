---
title: "Gaussian models, Part I"
subtitle: "Using simulation to understand generalized linear models"
date: "2023-02-10"
author: "Shane A. Scaggs"
categories: 
  - modeling
  - GLM
  - simulation
  - statistics
  - beginner
draft: true
---

```{r, echo=F, message=F, warning=F}
library(tidyverse)
ptheme = theme( panel.background = element_rect(color='black', fill='white'), 
                panel.grid = element_blank(), 
                strip.background = element_rect(color=NA,fill='white'),
                axis.ticks.length = unit(2, 'mm'))

```


# Prologue 

Conventional statistics courses that I've taken in the past have been incredibly unsatisfying. The material was dry and technical and this made only a very small portion of the content seem all that practical. 

I believe the essential problem with most conventional statistics courses is that they tend to emphasize statistical theory over statistical *practice*. This makes doing statistics feel very static, despite the reality that data analysis is a dynamic, iterative process. 

Fast forward to 2023 and I'm having students and colleagues ask me modeling questions that they clearly didn't have answered in their analytical training. They want to know which "test" to run and which buttons to click. Each conversation I have like this gives more motivation to help researchers shift from testing to *modeling*. 

The goal of this post is to start developing some primers for statistical models. Each post in this series will use simulation to understand a different model family in generalized linear modeling. 

# Background

Whether I'm working on Bayesian or frequentist models, the analytical workflow that I use today is based on the [free lectures](https://www.youtube.com/@rmcelreath) provided by Richard McElreath based on his book *Statistical Rethinking*. They are fantastic and mind opening. 

# Getting started 

Let's begin by just thinking about the mechanics of a linear model. First let's look at the equation for such a model: 

$$Y_i = \beta_0 + \beta_1 X_i$$
This equation looks more formidable than it is. The placeholders $X_i$ and $Y_i$ represent variables. These are values that we will include in the model from our data. The little $i$ is an index; you can sort of think of it like row $i$. When you are viewing an equation, you can look for these indices to determine which parameters of the equation are likely to vary. 

The greek letters are parameters that we estimate. In this case, $\beta_0$ is an intercept parameter -- the expected mean value when $X_i$ is 0. You'll also see this parameter symbolized with the greek letter $\alpha$. The $\beta_1$ parameter is a slope, a scalar that influences the relationship between $X_i$ and $Y_i$. 

# Simulating variables 

We won't be working with data at the moment. Instead we will simulate. So let's start with $X$. Let's imagine we have a variable with a mean value of 10 and a standard deviation of 2. 

```{r}
N = 400
x = rnorm( n=N, mean=10, sd=2 )
```

Having generated this variable `x` using random numbers from a Gaussian distribution, we get something similar to a bell shaped curve. 

```{r, echo=F, fig.width=4, fig.height=3.5}
ggplot( data = data.frame(x) ) + stat_density( aes(x=x), adjust=0.5, fill='#ff110033' ) + geom_vline( xintercept = 10, lty=2 ) + ptheme
```
Now let's do the same for $Y$, using a mean of 7 and a standard deviation of 1.5. 

```{r}
y = rnorm( n=N, mean=7, sd=1.5 )
```

Now what happens if we plot the association between these two variables? 

```{r, echo=F, fig.width=4, fig.height=3.5}
ggplot( data.frame(x,y) ) + geom_point( aes(x=x, y=y), stroke=3, size=1, pch=21, color='#ff110033' ) + ptheme
```
Because both `x` and `y` were generated independent of each other, there is no discernible relationship between them. This implies that the estimated value of $\beta_1$ will be near 0. We can confirm this with a model. 

```{r}
d = data.frame(x,y)
model1 = glm( formula = y ~ x, data=d, family = gaussian ) 
summary(model1)
```

Indeed, the coefficient esimate for `x` --  the slope -- is just about 0. And because the relationship is pretty much non-existent, the intercept parameter $\beta_0$ is estimated to be approximately the mean value for `y`. 

# Form a relationship 

So how would we go about linking `x` and `y`? Well we need to provide value for the slope parameter. I find this a bit easier to think about if we write a function. 

```{r}
linear = function( x, b0, b1 ) {
    y = b0 + b1*x
    data.frame(x,y)
}
```

Now suppose we want there to be a negative relationship between `x` and `y`. We need to provide a negative value for `b1`. 

```{r}
x = rnorm( n=N, mean=10, sd=2 )
sim1 = linear( x=x, b0 = 7, b1 = -1.5 )
```

Notice that we dont actually provide a vector of Y values this time. This is because they are generated by the function, rather than by sampling from a Gaussian distribution. 

Let's plot these results and see what we find. 

```{r, echo=F, fig.width=4, fig.height=3.5}
ggplot( data=sim1 ) + geom_point( aes(x=x, y=y), stroke=3, size=1, pch=21, color='#ff110033' ) + ptheme
```

So we have enforced a perfect linear relationship between `x` and `y`. But wouldn't we have expected a bit more variation rather than a perfect line? Indeed, in a sense we have lost the `sd = 1.5` that we originally included when we generated `y` the first time. To include it, we need a different equation. 

$$Y_i = \beta_0 + \beta_1 X_i + e_i$$
This term $e_i$ is an "error" term. It is the residual deviance that is left unexplained by the other parameters in the equation. 

Let's think about this another way. The original mean and sd used in `y = rnorm(N, 7, 1.5)` has been moved to the $\beta_0$ and the $e_i$. We no longer supply the mean, but instead we estimated it as $\beta_0$ with some residual error, $e_i$ left over. 

Let's amend our function. 

```{r}
linear = function( x, b0, b1, e ) {
    y = b0 + b1*x + e
    data.frame(x,y)
}
```

Now let's supply a constant value for `e` in this function; how about 1.5 like we used before? 

```{r}
x = rnorm( n=N, mean=10, sd=2 )
sim1 = linear( x=x, b0 = 7, b1 = -1.5, e = 2 )
```

```{r, echo=F, fig.width=4, fig.height=3.5}
ggplot( data=sim1 ) + geom_point( aes(x=x, y=y), stroke=3, size=1, pch=21, color='#ff110033' ) + ptheme
```
That didn't really work did it. Why not? Well if we look a bit closer, we see that all that has happened is the value 1.5, which we supplied for `e`, was just added on at the end. This is effectively like setting `b0` to 8.5 instead of 7. What we really need is a distribution of error, centered on 0 and deviating by 1.5. 
```{r}
x = rnorm( n=N, mean=10, sd=2 )
e = rnorm( n=N, mean=0, sd=1.5 )
sim1 = linear( x=x, b0 = 7, b1 = -1.5, e=e )
```

```{r, echo=F, fig.width=4, fig.height=3.5}
ggplot( data=sim1 ) + geom_point( aes(x=x, y=y), stroke=3, size=1, pch=21, color='#ff110033' ) + ptheme
```
That looks a bit more like what we might expect. But if it really is true that we're just adding some error onto the end, then we could get the same result if we don't use `e` at all and instead supply our original `y` generated with `rnorm` in place of `b0`. 

Set `e` to 0 and supply a distribution of values for `b0`.

```{r}
x = rnorm( n=N, mean=10, sd=2 )
sim1 = linear( x=x, b0 = rnorm( n=N, mean=7, sd=1.5), b1 = -1.5, e=0 )
```

```{r, echo=F, fig.width=4, fig.height=3.5}
ggplot( data=sim1 ) + geom_point( aes(x=x, y=y), stroke=3, size=1, pch=21, color='#ff110033' ) + ptheme
```

In future example, we might return to this approach as a way to created *random intercepts*. Right now, this just shows that the generative process will work the same with either approach. However, in practice, it will still be useful to use `e` to think about error.  

# Fit the model again 

Now that we have out generated data set, let's try rerunning the `glm` from above. 

```{r}
model1 = glm( formula = y ~ x, data=sim1, family = gaussian ) 
summary(model1)
```

This time, we recover our slope parameter $\beta_1$ and we more accurately recover our intercept parameter $\beta_0$. 

# Power 

The power of this technique is in our ability to run multiple simulation with different values. Let's try this with different values for `b1`. 

```{r}
x = rnorm( n=N, mean=10, sd=2 )
e = rnorm( n=N, mean=0, sd=1.5 )
b1 = seq( from=-3, to=3, length=12 )

L = list()
for( i in seq_along(b1) ) {
    out = linear( x=x, b0 = 7, b1 = b1[i], e=e )
    out$b1 = round(b1[i], 2)
    L[[i]] = out
}
```

In the code above, I've done many of the same things that I did above. The only difference is that I have create a vector of `b1` values ranging from -3 to +3, and I've looped over these values, plugging in each element of the vector `b1` into the linear equation. 

I stored all my results in a list. Each element of the list is a data frame containing the values of `x`, `y`, along with the value of `b1`. Let's look at one of them. 

```{r}
head(L[[1]])
```

Now if we used the `bind_rows` function from `{dplyr}`, we can change this list into a long form data frame where our results are grouped by the iteration. This will enable some fancy plotting. 

```{r}
ldf = bind_rows(L)
```

Now let's look at every relationship. This time I'll show the plotting code so you can see how it's done. 

```{r, fig.width=6, fig.height=8}
ggplot( data=ldf, aes(x=x, y=y) ) + ptheme + 
    geom_point( stroke=2, size=1, pch=21, color='#ff110011') + 
    facet_wrap(~b1, nrow = 4)
```
We can immediately see how change the values of `b1` has altered the relationship. 

Now let's do one more version of this, but instead, we will loop through different values of `e`, creating more or less error, while holding `b1` constant.  

```{r, fig.width=6, fig.height=8}
x = rnorm( n=N, mean=10, sd=2 )
e = seq( from=0.1, to=5, length=12)

L = list()
for( i in seq_along(e) ) {
    out = linear( x=x, b0 = 7, b1 = 2, e= rnorm(n=N, mean=0, sd = e[i]) )
    out$e = round(e[i], 2)
    L[[i]] = out
}
ldf = bind_rows(L)
ggplot( data=ldf, aes(x=x, y=y) ) + ptheme + 
    geom_point( stroke=2, size=1, pch=21, color='#ff110033') + 
    facet_wrap(~e, nrow = 4)
```

Now we see how changing the value `e` greatly increased the level of deviance for each of these simulated relationships. 
