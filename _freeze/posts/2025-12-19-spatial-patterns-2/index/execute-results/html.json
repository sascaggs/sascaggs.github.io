{
  "hash": "8259d1c868c7571b7339297be93bcd81",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Spatial Patterns II\"\nsubtitle: \"The Ising and Potts Models\"\ndate: 2025-12-20\ncategories: [landscape ecology, pattern generation, Ising model]\nformat:\n  html:\n    toc: true\nexecute: \n  warning: false\n  message: false\nbibliography: 2025-12-19.bib\n---\n\n# Introduction\n\nAs I have been learning about spatial patterns, I've become more interested in model families and the characteristic geometries that these families generate. For example, which models can produces spots? Clusters? Waves? Labyrinths?\n\n# Ising and Potts Models\n\nThe Ising/Potts model defines a probability distribution over spatial configurations composed of discrete states. In it's simplest form (the Ising Model)\n\n$$\nP(s) = \\frac{1}{Z} \\exp (-\\beta H(s))\n$$ {#eq-ising}\n\nthe state configuration $P(s)$ depends on a Hamiltonian energy $H(s)$. This energy arises from neighborhood interactions\n\n$$\nH(s) = -J\\sum_{\\langle i, j\\rangle} \\delta(s_i,s_j)\n$$ {#eq-ising-hamiltonian}\n\nwhere $\\langle i, j \\rangle$ identifies the neighboring sites. The Kronecker $\\delta$ returns a 1 when the states of two neighbors are the same ($s_i = s_j$), and a 0 when they are not ($s_i \\ne s_j$). For the Ising Model, $s_i \\in \\{-1,+1\\}$ which can then be written simply has $s_i s_j$. For the Potts model, $s_i \\in \\{1,...,q\\}$.\n\nThe parameter $\\beta$ controls the degree of stochasticity in the energetic alignment (e.g., temperature), and the parameter $J$ determines the strength of the alignment.\n\nTo simulate such a system, we first begin with random states on a lattice. This is our initial $s$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nN = 50\nS = matrix(\n  sample(c(-1,1), N*N, replace = T), \n  N, N\n) \n\nexpand.grid(i=1:N, j=1:N) |> \n  mutate(s = as.vector(S)) |> \n  ggplot(aes(i,j, fill=s)) + \n  geom_tile() + \n  theme_void() + \n  theme(legend.position = 'none') + \n  coord_fixed() + \n  scale_fill_gradientn(colors = c('#002244','#ccaa66'))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nWe then randomly select a focal state somewhere on the lattice\n\n\n::: {.cell}\n\n```{.r .cell-code}\ni = sample.int(N,1)\nj = sample.int(N,1)\n```\n:::\n\n\nand then identify the neighbors of $i$ and $j$. In this case, we identify the von Neumann neighborhood. This is done using some indexing tricks to deal with indexing that begins at 1 instead of 0, and to ensure that the neighborhood wraps around the lattice (no edge effects).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneighbor_sum = function(i,j,S) {\n  up    = S[(i-2) %% N+1, j ]\n  down  = S[ i    %% N+1, j ]\n  left  = S[ i, (j-2) %% N+1]\n  right = S[ i,  j    %% N+1]\n  up + down + left + right\n}\n```\n:::\n\n\nWhen combined with $J$ this gives $H(s)$, which can then be fed into equation @eq-ising to obtain $P(s)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJ = 1\nH = J * neighbor_sum(i,j,S) \nB = 0.7\nP = 1 / (1 + exp(-2 * B * H))\n```\n:::\n\n\nIt is worth noting that the function to compute $P(s)$ takes the general form $1 / (1 + e^{x})$, which is a logistic transform, otherwise known as the *binary softmax*. This is not a defining feature of the model. It arises out of necessity when going from discrete states into probabilities. It is the plumbing, and not the flow.\n\nThe flow comes from the local neighborhood configurations $s_is_j$ combined with $\\beta$ and $J$.\n\nThe final step then is to generate the new state given the probability.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nifelse(runif(1) < P, 1, -1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\nPutting this together, we have a function with which we can loop over focal states and then make updates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nising = function(N, beta = 0.7, J = 1, n_steps) {\n  \n  # states \n  S = matrix(sample(c(-1,1), N*N, replace = T), N, N)\n  \n  # neighbor sum \n  neighbor_sum = function(i,j,S) {\n    up    = S[(i-2) %% N+1, j ]\n    down  = S[ i    %% N+1, j ]\n    left  = S[ i, (j-2) %% N+1]\n    right = S[ i,  j    %% N+1]\n    up + down + left + right\n  }\n  \n  # loop \n  for(t in seq_len(n_steps)) {\n    i = sample.int(N,1)\n    j = sample.int(N,1)\n    H = J * neighbor_sum(i,j,S) \n    P = 1 / (1 + exp(-2 * beta * H))\n    S[i,j] = ifelse(runif(1) < P, 1, -1)\n  }\n  \n  S\n}\n```\n:::\n\n\n$J$ has a significant effect on the configuration that emerges over a sequence of updates. When $J > 0$, we observe a facilitation effect where neighbors of the same state grow into cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\nset.seed(888)\nN = 50\nJ = 1\nruntimes = c(1,100,1000,10000)\nplots = list()\nfor(r in 1:length(runtimes)) {\n  Ps = ising(N = N, \n             beta = 0.6, \n             J = J, \n             n_steps = runtimes[r]\n  )\n  \n  plot = expand.grid(i = 1:N, j = 1:N) |> \n    mutate(Ps = as.vector(Ps)) |> \n    ggplot(aes(i,j, fill=Ps)) + \n    geom_tile() + \n    theme_void() + \n    theme(legend.position = 'none') + \n    coord_fixed() +\n    labs(title = paste0('steps = ', runtimes[r], '\\nJ = ',J)) + \n    scale_fill_gradientn(colors = c('#000000','#ccaa66'))\n  plots[[r]] = plot\n}\n\n\nplots[[1]] + plots[[2]] + plots[[3]] + plots[[4]] + \n  plot_layout(design = 'ABCD')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=624}\n:::\n:::\n\n\nHowever, when $J < 0$ there is an inhibitory effect in which neighbors repel one another, locking into an interspersed configuration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJ = -1\nplots = list()\nfor(r in 1:length(runtimes)) {\n  Ps = ising(N = N, \n             beta = 0.6, \n             J = J, \n             n_steps = runtimes[r]\n  )\n  \n  plot = expand.grid(i = 1:N, j = 1:N) |> \n    mutate(Ps = as.vector(Ps)) |> \n    ggplot(aes(i,j, fill=Ps)) + \n    geom_tile() + \n    theme_void() + \n    theme(legend.position = 'none') + \n    coord_fixed() +\n    labs(title = paste0('steps = ', runtimes[r], '\\nJ = ',J)) + \n    scale_fill_gradientn(colors = c('#000000','#ccaa66'))\n  plots[[r]] = plot\n}\n\n\nplots[[1]] + plots[[2]] + plots[[3]] + plots[[4]] + \n  plot_layout(design = 'ABCD')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=624}\n:::\n:::\n\n\n## A useful extension\n\nOne way to extend this simulation is to allow more flexible neighborhoods. We do this by defining offsets, and use these to index the neighbors for a focal state.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_neighbors = function(offsets, N) {\n  \n  function(i, j) {\n    ii = (i + offsets[,1] - 1) %% N + 1\n    jj = (j + offsets[,2] - 1) %% N + 1\n    cbind(ii, jj)\n  }\n}\n```\n:::\n\n\n`make_neighbors` is a function factory that takes in the offsets (a vector of locations around a focal state) and the size of the lattice `N`, and provides a function that can lookup this neighborhood for a given $i$ and $j$ (much like the first portion of `neighbor_sum`). This makes the simulation much more flexible.\n\nA fun example is to generate disk offsets. Rather than indexing directions by hand, we can define a disk with radius `rd`. For instance, if `rd = 1`, then we get the familiar von Neumann neighborhood.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrd = 1\ndisk = as.matrix(expand.grid(-rd:rd, -rd:rd))\ndisk = disk[rowSums(disk^2) <= rd^2 & rowSums(disk^2) > 0,]\nas.data.frame(disk) |>\n  mutate(z = 1) |> \n  ggplot(aes(Var1,Var2, fill=z)) + \n  geom_tile(fill='tomato', alpha=0.75) + \n  theme_void() + \n  coord_fixed() + \n  theme(legend.position = 'none', \n        panel.grid = element_line(color='black'))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nIncreasing `rd` gives ever broader neighborhoods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_seq = c(1,2,3,4,5)\nplots = list()\nfor(i in 1:length(r_seq)) {\n  rd = r_seq[i]\n  disk = as.matrix(expand.grid(-rd:rd, -rd:rd))\n  disk = disk[rowSums(disk^2) <= rd^2 & rowSums(disk^2) > 0,]\n  \n  plot = as.data.frame(disk) |>\n    mutate(z = 1) |> \n    ggplot(aes(Var1,Var2, fill=abs(Var1*Var2))) + \n    geom_tile() + \n    theme_void() + \n    coord_fixed() + \n    xlim(c(-length(r_seq)-1,length(r_seq)+1)) + \n    ylim(c(-length(r_seq)-1,length(r_seq)+1)) + \n    scale_fill_gradientn(colors = c('#000055','#ff9966')) + \n    theme(legend.position = 'none', \n          panel.grid = element_line(color='grey70'), \n          plot.title = element_text(hjust=0.5)) + \n    labs(title = paste0('rd = ',rd))\n  plots[[i]] = plot\n}\n\nplots[[1]] + plots[[2]] + plots[[3]] + plots[[4]] + plots[[5]] + \n  plot_layout(design = 'ABCDE')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nHere we provides functions to make neighbors based on offsets, generate disk offsets, and then update the `ising` function to include these extensions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# neighborhood \nmake_neighbors = function(offsets, N) {\n  \n  function(i, j) {\n    ii = (i + offsets[,1] - 1) %% N + 1\n    jj = (j + offsets[,2] - 1) %% N + 1\n    cbind(ii, jj)\n  }\n}\n\n# disk function \ndisk_offsets = function(rd) {\n  disk = as.matrix(expand.grid(-rd:rd, -rd:rd))\n  disk = disk[rowSums(disk^2) <= rd^2 & rowSums(disk^2) > 0,]\n  return(disk)\n}\n\nising = function(N, beta = 0.7, J = 1, \n                 n_steps, offsets) {\n  \n  # states \n  S = matrix(sample(c(-1,1), N*N, replace = T), N, N)\n  \n  # neighborhood function\n  neighbors = make_neighbors(offsets, N)\n  \n  # loop \n  for(t in seq_len(n_steps)) {\n    i = sample.int(N,1)\n    j = sample.int(N,1)\n    n = neighbors(i,j)\n    \n    # hamiltonian and partition\n    H = J * sum(S[n[,1], n[,2]]) \n    P = 1 / (1 + exp(-2 * beta * H))\n    S[i,j] = ifelse(runif(1) < P, 1, -1)\n  }\n  \n  S\n}\n```\n:::\n\n\n## Potts Example\n\nNow let's turn to the Potts model. This code looks very simulate to the previous, except that instead of summing across all the states, we use `tabulate` to retrieve the frequency of the states within the neighborhood. All other changes are to extend the model to `q` states (e.g., `seq_len(q)`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npotts = function(N = 50, \n                 q = 5, \n                 beta = 0.5, \n                 J = 1, \n                 n_steps = 1e4, \n                 offsets) {\n  \n  S = matrix(sample(seq_len(q), N*N, replace = T), N, N)\n  neighbors = make_neighbors(offsets, N)\n  \n  for(t in seq_len(n_steps)) {\n    i = sample.int(N,1)\n    j = sample.int(N,1)\n    n = neighbors(i,j)\n    \n    neighbor_states = S[n[,1],n[,2]]\n    counts = tabulate(neighbor_states, nbins = q)\n    \n    p = exp(beta * J * counts)\n    p = p / sum(p)\n    \n    S[i,j] = sample(seq_len(q), 1, prob = p)\n  }\n  \n  S\n  \n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN = 100\nbeta = 0.3\nJ = 1\nmax_T = 1e4\nPs = potts(N=N, beta=beta, J=J, \n           n_steps = max_T, offsets = disk_offsets(rd=3))\n\np1 = expand.grid(i=1:N, j=1:N) |> \n  mutate(Ps = as.vector(Ps)) |> \n  ggplot(aes(i,j)) + \n  geom_tile(aes(fill=Ps)) + \n  theme_void() + \n  theme(legend.position = 'none', \n        plot.title = element_text(hjust = 0.5)) + \n  coord_fixed() + \n  labs(title = 'J = 1') + \n  scale_fill_gradientn(colours = c('#000000','#3300ff','tomato','#cccc66'))\n\nJ = -1\nPs = potts(N=N, beta=beta, J=J, \n           n_steps = max_T, offsets = disk_offsets(rd=3))\n\np2 = expand.grid(i=1:N, j=1:N) |> \n  mutate(Ps = as.vector(Ps)) |> \n  ggplot(aes(i,j)) + \n  geom_tile(aes(fill=Ps)) + \n  theme_void() + \n  theme(legend.position = 'none', \n        plot.title = element_text(hjust = 0.5)) + \n  coord_fixed() + \n  labs(title = 'J = -1') + \n  scale_fill_gradientn(colours = c('#000000','#3300ff','tomato','#cccc66'))\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n# Complete Model Notes\n\nThe Ising and Potts models generate a probability distribution over discrete spatial configurations using energy-based (Gibbs) neighborhood interactions [@wu1982potts]. Spatial patterns emerge from these local interactions. To simulate these models, we use local stochastic updates which typically converge to a stationary distribution. Spatial configurations exhibit an apparent \"criticality\" for a give size of the lattice on which the patterns are generated.\n\n## Core equations\n\nBoth the Ising and Potts models are based on the following equation\n\n$$\nP(s) = \\frac{1}{Z(\\beta,J)} \\exp \\biggl( \\beta J \\sum_{\\langle i,j \\rangle} 1(s_i = s_j) \\biggr)\n$$\n\nwhere $s$ is the full configuration, $\\langle i,j \\rangle$ are the neighbors, $J$ is the interaction strength, $\\beta$ is the inverse temperature (the decision sharpness/stochasticity), and $Z$ is a partition that normalizes the states to probabilities.\n\nFor the Ising model, $s_i \\in \\{-1,+1\\}$ while in the Potts model $s_i \\in \\{1,...,q\\}$.\n\nIn order to update the state of a focal cell at each step, the sum of the neighbor states must be transformed to a probability. For the Ising model, this is a logistic transform or a binary softmax\n\n\\begin{align}\n\nh_i &= \\sum_{j \\in \\partial_i} s_j \\\\\n\nP(s_i = +1 | s_{\\partial_i}) &= \\frac{1}{1 + \\exp (-2\\beta J h_i)} \n\n\\end{align}\n\nwhereas in the Potts model, it is a multinomial softmax\n\n\\begin{align}\nc_k &= \\sum_{j\\in \\partial_i} 1(s_j = k) \\\\\nP(s_i = k | s_{\\partial_i}) &= \\frac{\\exp(\\beta J c_k)}{\\sum^q_{\\ell=1} \\exp(\\beta J c_{\\ell})} \n\\end{align}\n\nHere the notation $\\partial_i$ denotes the neighborhood distance (e.g., `rd`) and all $j$ who fall into this neighborhood. Such transformations have no definitive influence on the spatial configurations; they merely normalize the probabilities [@newman1999monte].\n\n## Stationarity and Phase Transitions\n\nBecause Gibbs updates on a lattice are an example of a Markov chain [@binder1992monte], there is a unique stationary distribution for any finite value of $\\beta$. When $\\beta$ is large or near criticality, mixing is slow, making the stationary distribution difficult to find without extended computation.\n\nBecause a true critical point is only defined when $N \\rightarrow \\infty$, in a fixed lattice, one can only find a pseudo-critical value $K^*(N)$ where $K = \\beta J$. At $K^*$, fluctuations are approximately critical. Importantly, different neighborhoods will change the nature of the interactions and this $K$ will shift.\n\nAt this pseduo-critical point, we observe scale-free spatial patterns and diverging correlation lengths. To find this point, we hold $N$ contant and then sweep through $\\beta J$. For each $K$, we sample the equilibrium magnetization values\n\n\\begin{align}\n\nm_1, ..., m_T\n\n\\end{align}\n\nand compute fluctation based summaries (means) at $\\langle m^2 \\rangle$ and $\\langle m^4 \\rangle$. $K^*$ occurs where the fluctuations are greatest.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}