{
  "hash": "6223c03c6201f4f3ae625d805da6a668",
  "result": {
    "markdown": "---\ntitle: \"Gaussian models, Part I\"\nsubtitle: \"Using simulation to understand generalized linear models\"\nauthor: \"Shane A. Scaggs\"\ncategories: \n  - modeling\n  - GLM\n  - simulation\n  - statistics\n  - beginner\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Prologue \n\nConventional statistics courses that I've taken in the past have been incredibly unsatisfying. The material was dry and technical and this made only a very small portion of the content seem all that practical. \n\nI believe the essential problem with most conventional statistics courses is that they tend to emphasize statistical theory over statistical *practice*. This makes doing statistics feel very static, despite the reality that data analysis is a dynamic, iterative process. \n\nFast forward to 2023 and I'm having students and colleagues ask me modeling questions that they clearly didn't have answered in their analytical training. They want to know which \"test\" to run and which buttons to click. Each conversation I have like this gives more motivation to help researchers shift from testing to *modeling*. \n\nThe goal of this post is to start developing some primers for statistical models. Each post in this series will use simulation to understand a different model family in generalized linear modeling. \n\n# Background\n\nWhether I'm working on Bayesian or frequentist models, the analytical workflow that I use today is based on the [free lectures](https://www.youtube.com/@rmcelreath) provided by Richard McElreath based on his book *Statistical Rethinking*. They are fantastic and mind opening. \n\n# Getting started \n\nLet's begin by just thinking about the mechanics of a linear model. First let's look at the equation for such a model: \n\n$$Y_i = \\beta_0 + \\beta_1 X_i$$\nThis equation looks more formidable than it is. The placeholders $X_i$ and $Y_i$ represent variables. These are values that we will include in the model from our data. The little $i$ is an index; you can sort of think of it like row $i$. When you are viewing an equation, you can look for these indices to determine which parameters of the equation are likely to vary. \n\nThe greek letters are parameters that we estimate. In this case, $\\beta_0$ is an intercept parameter -- the expected mean value when $X_i$ is 0. You'll also see this parameter symbolized with the greek letter $\\alpha$. The $\\beta_1$ parameter is a slope, a scalar that influences the relationship between $X_i$ and $Y_i$. \n\n# Simulating variables \n\nWe won't be working with data at the moment. Instead we will simulate. So let's start with $X$. Let's imagine we have a variable with a mean value of 10 and a standard deviation of 2. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nN = 400\nx = rnorm( n=N, mean=10, sd=2 )\n```\n:::\n\n\nHaving generated this variable `x` using random numbers from a Gaussian distribution, we get something similar to a bell shaped curve. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=384}\n:::\n:::\n\nNow let's do the same for $Y$, using a mean of 7 and a standard deviation of 1.5. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ny = rnorm( n=N, mean=7, sd=1.5 )\n```\n:::\n\n\nNow what happens if we plot the association between these two variables? \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=384}\n:::\n:::\n\nBecause both `x` and `y` were generated independent of each other, there is no discernible relationship between them. This implies that the estimated value of $\\beta_1$ will be near 0. We can confirm this with a model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nd = data.frame(x,y)\nmodel1 = glm( formula = y ~ x, data=d, family = gaussian ) \nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ x, family = gaussian, data = d)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.60297    0.36661   18.01   <2e-16 ***\nx            0.04311    0.03623    1.19    0.235    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.011392)\n\n    Null deviance: 803.38  on 399  degrees of freedom\nResidual deviance: 800.53  on 398  degrees of freedom\nAIC: 1418.7\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\nIndeed, the coefficient esimate for `x` --  the slope -- is just about 0. And because the relationship is pretty much non-existent, the intercept parameter $\\beta_0$ is estimated to be approximately the mean value for `y`. \n\n# Form a relationship \n\nSo how would we go about linking `x` and `y`? Well we need to provide value for the slope parameter. I find this a bit easier to think about if we write a function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear = function( x, b0, b1 ) {\n    y = b0 + b1*x\n    data.frame(x,y)\n}\n```\n:::\n\n\nNow suppose we want there to be a negative relationship between `x` and `y`. We need to provide a negative value for `b1`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = rnorm( n=N, mean=10, sd=2 )\nsim1 = linear( x=x, b0 = 7, b1 = -1.5 )\n```\n:::\n\n\nNotice that we dont actually provide a vector of Y values this time. This is because they are generated by the function, rather than by sampling from a Gaussian distribution. \n\nLet's plot these results and see what we find. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=384}\n:::\n:::\n\n\nSo we have enforced a perfect linear relationship between `x` and `y`. But wouldn't we have expected a bit more variation rather than a perfect line? Indeed, in a sense we have lost the `sd = 1.5` that we originally included when we generated `y` the first time. To include it, we need a different equation. \n\n$$Y_i = \\beta_0 + \\beta_1 X_i + e_i$$\nThis term $e_i$ is an \"error\" term. It is the residual deviance that is left unexplained by the other parameters in the equation. \n\nLet's think about this another way. The original mean and sd used in `y = rnorm(N, 7, 1.5)` has been moved to the $\\beta_0$ and the $e_i$. We no longer supply the mean, but instead we estimated it as $\\beta_0$ with some residual error, $e_i$ left over. \n\nLet's amend our function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear = function( x, b0, b1, e ) {\n    y = b0 + b1*x + e\n    data.frame(x,y)\n}\n```\n:::\n\n\nNow let's supply a constant value for `e` in this function; how about 1.5 like we used before? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = rnorm( n=N, mean=10, sd=2 )\nsim1 = linear( x=x, b0 = 7, b1 = -1.5, e = 2 )\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=384}\n:::\n:::\n\nThat didn't really work did it. Why not? Well if we look a bit closer, we see that all that has happened is the value 1.5, which we supplied for `e`, was just added on at the end. This is effectively like setting `b0` to 8.5 instead of 7. What we really need is a distribution of error, centered on 0 and deviating by 1.5. \n\n::: {.cell}\n\n```{.r .cell-code}\nx = rnorm( n=N, mean=10, sd=2 )\ne = rnorm( n=N, mean=0, sd=1.5 )\nsim1 = linear( x=x, b0 = 7, b1 = -1.5, e=e )\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=384}\n:::\n:::\n\nThat looks a bit more like what we might expect. But if it really is true that we're just adding some error onto the end, then we could get the same result if we don't use `e` at all and instead supply our original `y` generated with `rnorm` in place of `b0`. \n\nSet `e` to 0 and supply a distribution of values for `b0`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = rnorm( n=N, mean=10, sd=2 )\nsim1 = linear( x=x, b0 = rnorm( n=N, mean=7, sd=1.5), b1 = -1.5, e=0 )\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=384}\n:::\n:::\n\n\nIn future example, we might return to this approach as a way to created *random intercepts*. Right now, this just shows that the generative process will work the same with either approach. However, in practice, it will still be useful to use `e` to think about error.  \n\n# Fit the model again \n\nNow that we have out generated data set, let's try rerunning the `glm` from above. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 = glm( formula = y ~ x, data=sim1, family = gaussian ) \nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ x, family = gaussian, data = sim1)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.37888    0.41167   17.92   <2e-16 ***\nx           -1.52464    0.03965  -38.46   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 2.223387)\n\n    Null deviance: 4172.91  on 399  degrees of freedom\nResidual deviance:  884.91  on 398  degrees of freedom\nAIC: 1458.8\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\nThis time, we recover our slope parameter $\\beta_1$ and we more accurately recover our intercept parameter $\\beta_0$. \n\n# Power \n\nThe power of this technique is in our ability to run multiple simulation with different values. Let's try this with different values for `b1`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = rnorm( n=N, mean=10, sd=2 )\ne = rnorm( n=N, mean=0, sd=1.5 )\nb1 = seq( from=-3, to=3, length=12 )\n\nL = list()\nfor( i in seq_along(b1) ) {\n    out = linear( x=x, b0 = 7, b1 = b1[i], e=e )\n    out$b1 = round(b1[i], 2)\n    L[[i]] = out\n}\n```\n:::\n\n\nIn the code above, I've done many of the same things that I did above. The only difference is that I have create a vector of `b1` values ranging from -3 to +3, and I've looped over these values, plugging in each element of the vector `b1` into the linear equation. \n\nI stored all my results in a list. Each element of the list is a data frame containing the values of `x`, `y`, along with the value of `b1`. Let's look at one of them. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(L[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          x         y b1\n1  9.398208 -24.44265 -3\n2  8.971316 -20.72967 -3\n3 10.411001 -24.99041 -3\n4  6.384421 -12.79901 -3\n5  9.752267 -22.44014 -3\n6 10.367787 -24.17492 -3\n```\n:::\n:::\n\n\nNow if we used the `bind_rows` function from `{dplyr}`, we can change this list into a long form data frame where our results are grouped by the iteration. This will enable some fancy plotting. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nldf = bind_rows(L)\n```\n:::\n\n\nNow let's look at every relationship. This time I'll show the plotting code so you can see how it's done. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot( data=ldf, aes(x=x, y=y) ) + ptheme + \n    geom_point( stroke=2, size=1, pch=21, color='#ff110011') + \n    facet_wrap(~b1, nrow = 4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=576}\n:::\n:::\n\nWe can immediately see how change the values of `b1` has altered the relationship. \n\nNow let's do one more version of this, but instead, we will loop through different values of `e`, creating more or less error, while holding `b1` constant.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = rnorm( n=N, mean=10, sd=2 )\ne = seq( from=0.1, to=5, length=12)\n\nL = list()\nfor( i in seq_along(e) ) {\n    out = linear( x=x, b0 = 7, b1 = 2, e= rnorm(n=N, mean=0, sd = e[i]) )\n    out$e = round(e[i], 2)\n    L[[i]] = out\n}\nldf = bind_rows(L)\nggplot( data=ldf, aes(x=x, y=y) ) + ptheme + \n    geom_point( stroke=2, size=1, pch=21, color='#ff110033') + \n    facet_wrap(~e, nrow = 4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=576}\n:::\n:::\n\n\nNow we see how changing the value `e` greatly increased the level of deviance for each of these simulated relationships. \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}