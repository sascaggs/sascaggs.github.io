{
  "hash": "8f29aca3a56cd79c062b7cdabfcc14c9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Food Web Generation and Measurement I\"\nsubtitle: \"Links and Detection\"\ndate: 2025-11-17\ncategories: [community ecology, food webs, hierarchical modeling]\nformat:\n  html:\n    toc: true\nbibliography: 2025-11-18.bib\n---\n\n### Introduction\n\nFood webs are often represented as network graphs, where nodes represent species and links represent observed feeding interactions, or potential feeding interactions inferred from species traits. The [Web of Life](https://www.web-of-life.es/) database houses many such graphs, showcasing their interesting structural patterns and the tremendous efforts ecologists have gone to in order to collect these challenging data.\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nA particular challenge when collecting food web data is knowing whether sampling effort is sufficient. Consider the implications of missing some feeding links. If an interaction involving even a single rare species goes unobserved, it can impact the overall network structure. Since many theories of food webs focus directly on explaining the structure of food webs, measurement models are needed that can help us infer whether the observed structure results from fundamental processes of community assembly or from partial sampling. In this blog post, we explore this measurement problem and develop some preliminary simulations and validations.\n\nTo begin, let's lay out a basic model. Let's assume that we want to infer species interactions using a hierarchical model (multiple food webs). This let's us pool information from across multiple webs to understand our ability to detect trophic links.\n\nFor each food web $k = 1,...,K$, we observe pairs of species interactions $A^{\\text{obs}}_{ij,k} \\in \\{0,1\\}$. A value of $1$ indicates the presence of a link between $i \\rightarrow j$. Given this, we model the probability of a link $\\pi_{ij,k}$ as\n\n$$\n\\begin{equation}\n\\text{logit}(\\pi_{ij,k}) = \\alpha_k + u_{i,k} + v_{j,k}\n\\end{equation}\n$$\n\nwhere $\\alpha_k$ is a baseline interaction rate for web $k$, and $u_{i,k}$ and $v_{j,k}$ are consumer- and resource-specific random effects. We could then say that the \"true\" set of interactions is Bernoulli distributed\n\n$$\n\\begin{equation}\nA^{\\text{obs}}_{ij,k} \\sim \\text{Bernoulli}(\\pi_{ij,k})\n\\end{equation}\n$$ In other words, there is a probability $\\pi_{ij,k}$ that we observe a link between $ij$ in food web $k$. Such a probability could be directly computed from $A$. But if sampling is biased, then this probability will inherit this bias. For this reason, we also need a detection model.\n\n$$\n\\begin{equation}\n\\text{logit}(p_k) = \\mu_p + \\sigma_p p^{(z)}_k\n\\end{equation}\n$$\n\nWe assume that each web has its own rate of detection $p_k$. The parameter $\\mu_p$ is the average log-odds of detection across all food webs with a specific deviation of web $k$ from the global average, which we model as non-centered: $\\sigma_p p^{(z)}_k$.\n\nIt follows that the probability of observing a link is\n\n$$\n\\begin{equation}\nq_{ij,k} = p_k \\cdot \\pi_{ij,k}\n\\end{equation}\n$$ We use this to amend our original likelihood to\n\n$$\n\\begin{equation}\nA^{\\text{obs}}_{ij,k} \\sim \\text{Bernoulli}(q_{ij,k})\n\\end{equation}\n$$\n\nIn essence, this model assumes that\n\n-   If $A^{\\text{true}}_{ij,k} = 0$, then $A^{\\text{obs}}_{ij,k} = 0$.\n-   If $A^{\\text{true}}_{ij,k} = 1$, then we observe $A^{\\text{obs}}_{ij,k} = 1$ with probability $\\pi_{ij,k}$.\n\nAlthough false positives are also a possible source of bias, we leave this out of the model, for now.\n\nFinally, we need to declare hierarchical, non-centered priors for all parameters, beginning first with the interaction rates, then the detection probabilities, and finally the random effects. We assume a weakly informative $\\mathcal{N}(0,1)$ prior on all latent deviations, and $\\text{Exponential}(1)$ for all SD.\n\n### Interaction rates\n\n$$\n\\begin{align}\n\\pi_{ij,k} &= \\text{logit}^{-1}(\\alpha_k + u_{i,k} + v_{j,k}) \\qquad &\\text{true link probability} \\\\\n\\alpha_k &= \\mu_{\\alpha} + \\sigma_{\\alpha} \\alpha^{(z)}_k \\qquad &\\text{non-centered} \\\\\n\\alpha^{(z)} &\\sim \\mathcal{N}(0,1) \\qquad &\\text{latent deviation} \\\\ \n\\mu_{\\alpha} &\\sim \\mathcal{N}(0,1) \\qquad &\\text{global mean interaction rate} \\\\ \n\\sigma_{\\alpha} &\\sim \\text{Exponential}(1) \\qquad &\\text{global interaction SD}\n\\end{align}\n$$\n\n### Detection\n\n$$\n\\begin{align}\np_k &= \\text{logit}^{-1}(\\mu_{p} + \\sigma_{p} p^{(z)}_{k}) \\qquad &\\text{non-centered} \\\\ \np^{(z)}_k &\\sim \\mathcal{N}(0,1) \\qquad &\\text{latent deviation} \\\\ \n\\mu_{p} &\\sim \\mathcal{N}(0,1) \\qquad &\\text{global mean detection rate} \\\\ \n\\sigma_{p} &\\sim \\text{Exponential}(1) \\qquad &\\text{global detection SD}\n\\end{align}\n$$\n\n### Consumer and resource random effects\n\n$$\n\\begin{align}\nu_{i,k} &= \\sigma_u u^{(z)}_{i,k}, \\qquad &u^{(z)}_{i,k} \\sim \\mathcal{N}(0,1) \\\\  \nv_{j,k} &= \\sigma_v v^{(z)}_{j,k}, \\qquad &v^{(z)}_{j,k} \\sim \\mathcal{N}(0,1) \\\\ \n\\sigma_u &\\sim \\text{Exponential}(1) \\qquad &\\sigma_v \\sim \\text{Exponential}(1)\n\\end{align}\n$$\n\n## Simulation\n\nTo test this model, we need to simulate food webs to serve as the \"true\" sets of interactions, and \"observe\" our food webs probabilistically from them. Here are the packages used in this workflow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(igraph)\nlibrary(patchwork)\nlibrary(latex2exp)\n```\n:::\n\n\nWe begin by setting up our parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ---- Setup ----\nset.seed(777)\ninv_logit = function(x) exp(x) / (1 + exp(x))\n\n# ---- Parameters ----\nK = 3   \nS = 30\nmu_alpha = -2\nsigma_alpha = 1\nsigma_u = 2\nsigma_v = 1\nmu_p = -1\nsigma_p = 0.5\n\n# generate varying base rates and detection probabilities \nalpha_k = rnorm(K, mu_alpha, sigma_alpha); alpha_k\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.510214 -2.398541 -1.489164\n```\n\n\n:::\n\n```{.r .cell-code}\np_k = inv_logit(rnorm(K, mu_p, sigma_p)); p_k\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2315809 0.4549582 0.3341748\n```\n\n\n:::\n:::\n\n\nWe then set up some containers for the simulation results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ---- Outputs ----\nA_true = array(0, dim = c(S,S,K))\nA_obs  = array(0, dim = c(S,S,K))\n```\n:::\n\n\nThe basic procedure is the following:\n\n-   Generate $u_{i,k}$ and $v_{j,k}$.\n-   Create $S \\times S$ matrices for the \"true\" and \"observed\" webs.\n-   Generate $\\pi_{ij,k}$ based on $\\alpha_k$ + random effects.\n-   Adjust observed links based on $p_k \\cdot \\pi_{ij,k}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(k in 1:K) {\n  \n  # Random effects\n  u_i = rnorm(S, mean = 0, sd = sigma_u)\n  v_j = rnorm(S, mean = 0, sd = sigma_v)\n  \n  # Loop \n  for(i in 1:S) for(j in 1:S) {\n    \n    pi_ijk = inv_logit(alpha_k[k] + u_i[i] + v_j[j])\n    \n    A_true[i,j,k] = rbinom(1, 1, pi_ijk)\n    A_obs[i,j,k] = rbinom(1, 1, p_k[k] * pi_ijk)\n  }\n}\n```\n:::\n\n\nThis yields the following summary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat = data.frame(\n  web            = 1:K, \n  true_links     = apply(A_true, 3, sum), \n  observed_links = apply(A_obs,  3, sum), \n  detection      = p_k, \n  alpha          = alpha_k\n); dat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  web true_links observed_links detection     alpha\n1   1        297             55 0.2315809 -1.510214\n2   2        124             49 0.4549582 -2.398541\n3   3        185             66 0.3341748 -1.489164\n```\n\n\n:::\n:::\n\n\nIf we examine the degree distributions across all of the webs, we can see how they differ after we adjust for the probability of detection.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndeg = function(A) apply(A, 3, function(mat) rowSums(mat)) |>  as.vector()\n\ndata.frame(\n  True      = deg(A_true),\n  Observed  = deg(A_obs)\n) |> \n  gather(key = key, value = degree) |> \n  ggplot(aes(degree)) + \n  geom_histogram(color='white', fill='black') + \n  facet_wrap(~key) + \n  labs(x = 'Outgoing links', y = 'Count')\n```\n\n::: {.cell-output-display}\n![The out-degree distribution across all simulated webs.](index_files/figure-html/unnamed-chunk-9-1.png){width=576}\n:::\n:::\n\n\nWe can see that the degree distribution of the observed links is more zero-inflated and that the tail of the distribution has contracted. Altering the values of $\\sigma_u$ and $\\sigma_v$ can change these properties to some extent, especially the heaviness of the tail.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_true1 = A_true[,,1]\nA_obs1  = A_obs[,,1]\n\nrow_sums = rowSums(A_true1)\nsort_order = order(row_sums)\n\npar(mfrow=c(1,2))\nimage(A_true1[sort_order, sort_order], \n      col = c('black','tomato'), \n      main = 'True', \n      xaxt = 'n', yaxt = 'n')\nimage(A_obs1[sort_order, sort_order],  \n      col = c('black','tomato'), \n      main = 'Observed', \n      xaxt = 'n', yaxt = 'n')\n```\n\n::: {.cell-output-display}\n![Examples of true and observed adjacency matrices.](index_files/figure-html/unnamed-chunk-10-1.png){width=624}\n:::\n:::\n\n\n### Model Validation\n\nThe next step is to develop the hierarchical model in Stan and validate it using the true and observed interaction matrices. First, we define the model block to mirror our simulation and the model notation. This block include the priors for all parameters and hyperparameters.\n\n``` stan\nmodel {\n  // priors for latent z-parameters \n  p_z ~ normal(0,1); \n  alpha_z ~ normal(0,1);\n  to_vector(u_z) ~ normal(0,1); \n  to_vector(v_z) ~ normal(0,1); \n  \n  //priors for hyperparameters \n  mu_p ~ normal(0,1);\n  sigma_p ~ exponential(1);\n  mu_alpha ~ normal(0,1);\n  sigma_alpha ~ exponential(1); \n  sigma_u ~ exponential(1);\n  sigma_v ~ exponential(1);\n  \n  // likelihood \n  for(k in 1:K) {\n    // detection random effect\n    real p_k = inv_logit(mu_p + sigma_p * p_z[k]);\n    \n    // link random effect\n    real alpha_k = mu_alpha + sigma_alpha * alpha_z[k]; \n    \n    for(i in 1:N_cons) {\n      // consumer random effects \n      real u_ik = sigma_u * u_z[i,k];\n      \n      for(j in 1:N_res) {\n        // resource random effect \n        real v_jk = sigma_v * v_z[j,k];\n        \n        // link probability \n        real pi_ijk = inv_logit(alpha_k + u_ik + v_jk);\n        \n        // detection adjusted probability \n        real q_ijk = p_k * pi_ijk;\n        \n        // likelihood\n        A[i,j,k] ~ bernoulli(q_ijk);\n      }\n    }\n  }\n}\n```\n\nFor all parameters that have priors, we declare them in the parameters block.\n\n``` stan\nparameters {\n  vector[K] p_z;\n  vector[K] alpha_z;\n  matrix[N_cons, K] u_z;\n  matrix[N_res, K] v_z;\n  \n  real mu_p; \n  real<lower=0> sigma_p; \n  real mu_alpha; \n  real<lower=0> sigma_alpha; \n  real<lower=0> sigma_u; \n  real<lower=0> sigma_v;\n}\n```\n\nFinally, we set up the data needed for the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_list = list(\n  N_cons = S, \n  N_res  = S, \n  K      = K, \n  A      = A_obs\n)\n```\n:::\n\n\nThis corresponds to\n\n``` stan\ndata {\n  int<lower=1> N_cons;\n  int<lower=1> N_res;\n  int<lower=1> K;\n  // observed adjacency\n  array[N_cons, N_res, K] int<lower=0, upper=1> A; \n}\n```\n\nView the full stan model.\n\n``` stan\n\ndata {\n  int<lower=1> N_cons;\n  int<lower=1> N_res;\n  int<lower=1> K;\n  // observed adjacency\n  array[N_cons, N_res, K] int<lower=0, upper=1> A; \n}\n\n\nparameters {\n  // non-centered latent effects\n  vector[K] p_z;\n  vector[K] alpha_z;\n  matrix[N_cons, K] u_z;\n  matrix[N_res, K] v_z;\n  \n  // hyperparameters\n  real mu_p; \n  real<lower=0> sigma_p; \n  real mu_alpha; \n  real<lower=0> sigma_alpha; \n  real<lower=0> sigma_u; \n  real<lower=0> sigma_v;\n}\n\nmodel {\n  // priors for latent z-parameters \n  p_z ~ normal(0,1); \n  alpha_z ~ normal(0,1);\n  to_vector(u_z) ~ normal(0,1); \n  to_vector(v_z) ~ normal(0,1); \n  \n  //priors for hyperparameters \n  mu_p ~ normal(0,1);\n  sigma_p ~ exponential(1);\n  mu_alpha ~ normal(0,1);\n  sigma_alpha ~ exponential(1); \n  sigma_u ~ exponential(1);\n  sigma_v ~ exponential(1);\n  \n  // likelihood \n  for(k in 1:K) {\n    // detection random effect\n    real p_k = inv_logit(mu_p + sigma_p * p_z[k]);\n    \n    // link random effect\n    real alpha_k = mu_alpha + sigma_alpha * alpha_z[k]; \n    \n    for(i in 1:N_cons) {\n      // consumer random effects \n      real u_ik = sigma_u * u_z[i,k];\n      \n      for(j in 1:N_res) {\n        // resource random effect \n        real v_jk = sigma_v * v_z[j,k];\n        \n        // link probability \n        real pi_ijk = inv_logit(alpha_k + u_ik + v_jk);\n        \n        // detection adjusted probability \n        real q_ijk = p_k * pi_ijk;\n        \n        // likelihood\n        A[i,j,k] ~ bernoulli(q_ijk);\n      }\n    }\n  }\n}\n```\n\nAfter saving this model, we fit it using `cmdstanr` like so. This model samples in under a minute with 4 parallel chains when `S = 30` and `K = 3`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cmdstanr)\nmod <- cmdstan_model(\"detection_model.stan\")\n\nfit1 <- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  adapt_delta = 0.95\n)\n\nfit$save_output_files()\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nOur primary task is to determine whether the model can recover the parameters that we used within the simulation. Two focal parameters are the baseline interaction probability $\\alpha_k$ and the detection probability $p_k$. As a reminder, we generated these values by setting a $\\mu*$ and $\\sigma*$ for each and then drawing them `K` times.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfocal_pars = data.frame(k = factor(1:k), alpha_k, p_k)\nfocal_pars\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  k   alpha_k       p_k\n1 1 -1.510214 0.2315809\n2 2 -2.398541 0.4549582\n3 3 -1.489164 0.3341748\n```\n\n\n:::\n:::\n\n\nSince we modeled these as non-centered parameters, we need to reconstruct them from the draws. We will do this using the `tidybayes` package for handling Bayesian models in R.[^1] Let's start with $\\alpha_k$. We will generate a predicted $\\hat{\\alpha}_k$ by pulling out `alpha_z[k]`, `mu_alpha`, and `sigma_alpha` and the computing `alpha_k_hat`.\n\n[^1]: Details of how to use `tidybayes` can be found [here](https://mjskay.github.io/tidybayes/articles/tidybayes.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidybayes)\n\nalpha_k_draws = fit1 |> \n  spread_draws(mu_alpha, sigma_alpha, alpha_z[k]) |>\n  group_by(k) |> \n  mutate(alpha_k_hat = mu_alpha + sigma_alpha * alpha_z) \n```\n:::\n\n\nWe can summarize the posterior distribution in many ways. Here I will plot this distribution and overlay the original values.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Comparison of predicted $\\hat{\\alpha}_k$ to the input values of $\\alpha_k$ (magenta $*$). Points are posterior means, and bars are 67\\% and 90\\% highest density posterior intervals.](index_files/figure-html/unnamed-chunk-16-1.png){width=408}\n:::\n:::\n\n\n$\\hat{\\alpha}_k$ falls within the density intervals in all cases, and comes extremely close to the mean for webs 1 and 3. We can now do this same to compute $\\hat{p}_k$, though in this case we need to transform the model parameters from the log-odds scale to a probability scale.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_k_draws = fit1 |> \n  spread_draws(p_z[k], sigma_p, mu_p) |> \n  group_by(k) |> \n  mutate(p_k_hat = inv_logit(mu_p + sigma_p*p_z))\n\np_k_draws |> \n  ggplot(aes(x=p_k_hat, y=factor(k))) + \n  stat_pointinterval(color='black', .width=c(0.67,0.9)) + \n  labs(x = TeX(\"$\\\\hat{p}_k$\"), y = 'k') + \n  geom_point(data=focal_pars, \n             aes(x = p_k, y=k), \n             color = 'white', pch=8, size=4, stroke=2) +\n  geom_point(data=focal_pars, \n             aes(x = p_k, y=k), \n             color = 'cyan3', pch=8, size=2, stroke=2)\n```\n\n::: {.cell-output-display}\n![Comparison of predicted $\\hat{p}_k$ to the input values of $p_k$ (cyan $*$). Points are posterior means, and bars are 67\\% and 90\\% highest density posterior intervals.](index_files/figure-html/unnamed-chunk-17-1.png){width=408}\n:::\n:::\n\n\nAgain we come very close to recovering the true parameters, with the biggest difference being in web 2. Although we can certainly be more precise, this is still a good sign that the model could, in principle, estimate the latent detection probability $p_k$.\n\n## Non-Random Links\n\nNow that we have a basic model and simulation skeleton, we should consider which traits might lead to a species having more or less feeding links. For example, we could expect that larger species may have broader diets. This idea has be presented many times, with notable examples coming from @petchey2008size and @allesina2011predicting. Here we can make a simplistic assumption that the mass of a consumer $M_i$ can have a linear, additive effect on the probability of a link.[^2]\n\n[^2]: @allesina2011predicting explore additional possibilities, such as the *Ratio* model, that we discuss in more detail in the next post.\n\n$$\n\\begin{equation}\n\\text{logit}(\\pi_{ij,k}) = \\alpha_k + (u_{i,k} + b M_i) + v_{j,k}\n\\end{equation}\n$$\n\nRather than having a single parameter $b$ for all webs, it is more likely that we have distinct allometric relationships per web, because each simulated web could represent a distinct ecological community. This implies a random slopes model:\n\n$$\n\\begin{align}\n\\text{logit}(\\pi_{ij,k}) &= \\alpha_k + (u_{i,k} + b_k M_{i,k}) + v_{j,k} \\\\\nb_k &= \\mu_b + \\sigma_b b^{(z)}_k \\\\ \n\\mu_b &\\sim \\mathcal{N}(0,1) \\\\\nb^{(z)}_k &\\sim \\mathcal{N}(0,1) \\\\\n\\sigma_b &\\sim \\text{Exponential}(1) \\\\ \n\\end{align}\n$$\n\nwhere $i \\in \\{1,...,S\\}$ and $k \\in \\{1,...,K\\}$. This means that $i$ in web $k = 1$ is not the same as $i$ in web $k = 2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ---- Setup ----\nset.seed(777)\ninv_logit = function(x) exp(x) / (1 + exp(x))\n\n# ---- Parameters ----\nK = 3   \nS = 30\nmu_alpha = -2\nsigma_alpha = 1\nsigma_u = 2\nsigma_v = 1\nmu_p = -1\nsigma_p = 0.5\nmu_b = 1\nsigma_b = 0.3\n\n# consumer mass \nM = matrix(NA_real_, nrow = S, ncol = K)\n\n# generate varying base rates and detection probabilities, and body mass effects \nalpha_k = rnorm(K, mu_alpha, sigma_alpha)\np_k = inv_logit(rnorm(K, mu_p, sigma_p))\nb_k = rnorm(K, mu_b, sigma_b)\n\n# ---- Outputs ----\nA_true = array(0, dim = c(S,S,K))\nA_obs  = array(0, dim = c(S,S,K))\nA_Mtrue = array(0, dim = c(S,S,K))\nA_Mobs  = array(0, dim = c(S,S,K))\n\n\nfor(k in 1:K) {\n  # Random effects\n  u_i = rnorm(S, mean = 0, sd = sigma_u)\n  v_j = rnorm(S, mean = 0, sd = sigma_v)\n  \n  \n    # Mass \n    mass_k = exp(rnorm(S, mu_b, sigma_b))\n    M[,k] = log(mass_k)\n  \n  # Loop \n  for(i in 1:S) for(j in 1:S) {\n    \n    pi_ijk = inv_logit(alpha_k[k] + u_i[i] + v_j[j])\n    A_true[i,j,k] = rbinom(1, 1, pi_ijk)\n    A_obs[i,j,k] = rbinom(1, 1, p_k[k] * pi_ijk)\n    \n    # body mass model \n    .pi_ijk = inv_logit(alpha_k[k] + (u_i[i] + b_k[k] * M[i,k]) + v_j[j])\n    A_Mtrue[i,j,k] = rbinom(1, 1, .pi_ijk)\n    A_Mobs[i,j,k] = rbinom(1, 1, p_k[k] * .pi_ijk)\n    \n  }\n}\n```\n:::\n\n\nClearly the choice of the distribution of body masses will have an influence over this simulation. Here I assume that body masses are heavy-tailed and that we model them on a log-scale. For example, the code above gives the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(mass_k) |>  \n  ggplot(aes(x=mass_k)) +  \n  stat_density(adjust = 0.5) + \n  labs(x=expression(M[i])) + \n  ggtitle('Raw mass') + \n\ndata.frame(M=M[,1]) |>  \n  ggplot(aes(x=M)) +  \n  stat_density(adjust = 0.5) + \n  labs(x=expression(log(M[i]))) + \n  ggtitle('Log transformed') \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/dpi-300-1.png){width=624}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_true1  = A_true[,,1]\nA_obs1   = A_obs[,,1]\nA_Mtrue1 = A_Mtrue[,,1]\nA_Mobs1  = A_Mobs[,,1]\n\nrow_sums = rowSums(A_true1)\nsort_order = order(row_sums)\n\npar(mfrow=c(2,2), mar=c(3,1,3,1))\nimage(A_true1[sort_order, sort_order], col = c('black','tomato'), main = 'True', xaxt = 'n', yaxt = 'n')\nimage(A_obs1[sort_order, sort_order],  col = c('black','tomato'), main = 'Observed', xaxt = 'n', yaxt = 'n')\n\nrow_sums = rowSums(A_Mtrue1)\nsort_order = order(row_sums)\n\nimage(A_Mtrue1[sort_order, sort_order], col = c('black','tomato'), main = 'True (Allometric)', xaxt = 'n', yaxt = 'n')\nimage(A_Mobs1[sort_order, sort_order],  col = c('black','tomato'), main = 'Observed (Allometric)', xaxt = 'n', yaxt = 'n')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=384}\n:::\n:::\n\n\nThe effect of body mass concentrates the true links toward the largest species. It is also gravitational: energy, in the form of biomass, flows toward these largest species. The effect of this on the observed links is very subtle but not entirely absent.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlo = layout_as_tree(graph_from_adjacency_matrix(A_Mobs1))\npar(mfrow=c(2,2), mar=c(3,1,3,1))\nplot(graph_from_adjacency_matrix(A_true1),  \n     edge.arrow.size=0.2, \n     vertex.color = 'white', \n     vertex.label = NA, \n     layout = lo, \n     main = 'True')\nplot(graph_from_adjacency_matrix(A_obs1),   \n     edge.arrow.size=0.2, \n     vertex.color = 'white', \n     vertex.label = NA, \n     layout = lo, \n     main = 'Observed')\nplot(graph_from_adjacency_matrix(A_Mtrue1), \n     edge.arrow.size=0.2, \n     vertex.color = 'white', \n     vertex.label = NA, \n     layout = lo, \n     main = 'True (Allometric)')\nplot(graph_from_adjacency_matrix(A_Mobs1),  \n     edge.arrow.size=0.2, \n     vertex.color = 'white', \n     vertex.label = NA, \n     layout = lo, \n     main = 'Observed (Allometric)')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=1950}\n:::\n:::\n\n\n### Stan model\n\nNow we adjust the original Stan model to include these biomass effects. The main changes, besides adding the non-centered effects of $b_k$, are to include the body mass matrix `M`.\n\n``` stan\n\ndata {\n  int<lower=1> N_cons;\n  int<lower=1> N_res;\n  int<lower=1> K;\n  matrix[N_cons,K] M;\n  // observed adjacency\n  array[N_cons, N_res, K] int<lower=0, upper=1> A; \n}\n\n\nparameters {\n  // body mass slopes \n  vector[K] b_z;\n  real mu_b;\n  real<lower=0> sigma_b;\n  \n  // non-centered latent effects\n  vector[K] p_z;\n  vector[K] alpha_z;\n  matrix[N_cons, K] u_z;\n  matrix[N_res, K] v_z;\n  \n  // hyperparameters\n  real mu_p; \n  real<lower=0> sigma_p; \n  real mu_alpha; \n  real<lower=0> sigma_alpha; \n  real<lower=0> sigma_u; \n  real<lower=0> sigma_v;\n}\n\nmodel {\n  // b priors\n  b_z ~ normal(0,1);\n  mu_b ~ normal(0,1); \n  sigma_b ~ exponential(1);\n  \n  // priors for latent z-parameters \n  p_z ~ normal(0,1); \n  alpha_z ~ normal(0,1);\n  to_vector(u_z) ~ normal(0,1); \n  to_vector(v_z) ~ normal(0,1); \n  \n  //priors for hyperparameters \n  mu_p ~ normal(0,1);\n  sigma_p ~ exponential(1);\n  mu_alpha ~ normal(0,1);\n  sigma_alpha ~ exponential(1); \n  sigma_u ~ exponential(1);\n  sigma_v ~ exponential(1);\n  \n  // likelihood \n  for(k in 1:K) {\n    // detection random effect\n    real p_k = inv_logit(mu_p + sigma_p * p_z[k]);\n    \n    // link random effect\n    real alpha_k = mu_alpha + sigma_alpha * alpha_z[k];\n    \n    // body mass slopes \n    real b_k = mu_b + sigma_b * b_z[k];\n    \n    for(i in 1:N_cons) {\n      // consumer random effects \n      real u_ik = sigma_u * u_z[i,k];\n      \n      for(j in 1:N_res) {\n        // resource random effect \n        real v_jk = sigma_v * v_z[j,k];\n        \n        // link probability \n        real pi_ijk = inv_logit(alpha_k + (u_ik + b_k*M[i,k]) + v_jk);\n        \n        // detection adjusted probability \n        real q_ijk = p_k * pi_ijk;\n        \n        // likelihood\n        A[i,j,k] ~ bernoulli(q_ijk);\n      }\n    }\n  }\n}\n```\n\nAs before, we fit the model but with slightly increased `adapt_delta` to accommodate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBmod <- cmdstan_model(\"detection_model_bodymass.stan\")\n\nfit2 <- modB$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  adapt_delta = 0.95\n)\n\nfit2$save_output_files()\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nAs before, we want to compare the predicted values of $\\hat{\\alpha}_k$, $\\hat{p}_k$, and $\\hat{b}_k$ to their input values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfocal_pars2 = data.frame(k = factor(1:k), \n                        alpha_k, p_k, b_k)\nfocal_pars2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  k   alpha_k       p_k       b_k\n1 1 -1.510214 0.2315809 1.0608113\n2 2 -2.398541 0.4549582 1.3326813\n3 3 -1.489164 0.3341748 0.9381326\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=624}\n:::\n:::\n\n\nAgain we see a decent concordance between the predicted and input values, although in some cases we are near the edge of the highest density posterior interval.\n\nNext, we examine the predicted $\\hat{b}_k$ from the posterior against the input values.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Comparison of the predicted $\\hat{b}_k$ values against the input values $b_k$.](index_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}